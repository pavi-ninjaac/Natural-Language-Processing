{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is about predicting the next word of the sentence, this can be told mathematical as\n",
    "\n",
    "P(wi+1 | wi, wi-1, wi-2) => predicting the word i+1 based on the words i, i-1, i-2 ...\n",
    "\n",
    "Real life examples:\n",
    "- In google search bar when you type some words it will show the completion\n",
    "- Whatsapp or other places when you typing, it will show the next word suggestion\n",
    "- Gmail word completion while writing the mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-05 11:32:03.928456: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-05 11:32:03.967353: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-05-05 11:32:03.967896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-05 11:32:04.733035: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# basic packages.\n",
    "from typing import Final\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import kaggle_utils as utils \n",
    "\n",
    "# visualization packages.\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# NLP framework.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "import gensim\n",
    "\n",
    "# model preprocessing and metrics.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================= Common data analysis =======================================\n",
      "\n",
      "Numerical data list [] ---> total 0 numerical values\n",
      "Categorical data list ['title'] ---> total 1 categorical values\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d68e4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d68e4_level0_col0\" class=\"col_heading level0 col0\" >data type</th>\n",
       "      <th id=\"T_d68e4_level0_col1\" class=\"col_heading level0 col1\" >Missing Value(NA)</th>\n",
       "      <th id=\"T_d68e4_level0_col2\" class=\"col_heading level0 col2\" >?[]na null ' ' </th>\n",
       "      <th id=\"T_d68e4_level0_col3\" class=\"col_heading level0 col3\" >% of Missing value(NA)</th>\n",
       "      <th id=\"T_d68e4_level0_col4\" class=\"col_heading level0 col4\" >% of Missing value(?[]na null ' ')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d68e4_level0_row0\" class=\"row_heading level0 row0\" >title</th>\n",
       "      <td id=\"T_d68e4_row0_col0\" class=\"data row0 col0\" >object</td>\n",
       "      <td id=\"T_d68e4_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_d68e4_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_d68e4_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_d68e4_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fb632da9d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>“We” vs “I” — How Should You Talk About Yourse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>How Donald Trump Markets Himself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>Content and Marketing Beyond Mass Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>5 Questions All Copywriters Should Ask Clients...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>How To Write a Good Business Blog Post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6508 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title\n",
       "0     A Beginner’s Guide to Word Embedding with Gens...\n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...\n",
       "2                          How to Use ggplot2 in Python\n",
       "3     Databricks: How to Save Files in CSV on Your L...\n",
       "4     A Step-by-Step Implementation of Gradient Desc...\n",
       "...                                                 ...\n",
       "6503  “We” vs “I” — How Should You Talk About Yourse...\n",
       "6504                   How Donald Trump Markets Himself\n",
       "6505      Content and Marketing Beyond Mass Consumption\n",
       "6506  5 Questions All Copywriters Should Ask Clients...\n",
       "6507             How To Write a Good Business Blog Post\n",
       "\n",
       "[6508 rows x 1 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the data\n",
    "file_path: str = \"data/medium_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head(5)\n",
    "\n",
    "# We are just gonna consider the title of the articles.\n",
    "data = data[[\"title\"]]\n",
    "\n",
    "# Basic data common analysis.\n",
    "column, categorical_data, numerical_data, missing_data = utils.Common_data_analysis(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "\n",
    "\n",
    "# can i remove punctuation: yes i have to\n",
    "# can i remove links: yes it is not gonna give any useful information here\n",
    "# can i remove numbers: yes, we can because \"2 people died through the disaster\" and  \"people died through the disaster\" are both gonna give the same meaning.\n",
    "# can i remove stop words: yes, we can.\n",
    "\n",
    "\n",
    "class PreprocessData:\n",
    "    \"\"\"\n",
    "    Class to preprocess the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the dataframe.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.column_name: str = column_name\n",
    "        self.file_name: str = \"pre-processed-data.pkl\"\n",
    "\n",
    "    def _remove_symbols(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the numbers.\n",
    "        * Removes punctuations.\n",
    "        * Removes any symbols.\n",
    "        \"\"\"\n",
    "        # 1 | remove the punctuation words and symbols and numbers.\n",
    "        # will create a regex and apply over the text.\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"[0123456789!\\\"\\'\\’\\“\\”\\—#$%&()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", sentence)) \n",
    "\n",
    "    def _remove_links(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes any links.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"(w+://S+)\", \" \", sentence))\n",
    "\n",
    "    def _case_fold_to_lower_case(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Change the sentence into lower case.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].str.lower()\n",
    "\n",
    "    def _fix_typo(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Fix the typo of the each word -- in real word when you get data from tweets, mails, articles etc\n",
    "        * there is a chance of typo since they are written by humans. We need to fix it to get better results.\n",
    "        \"\"\"\n",
    "        # we are using \n",
    "        pass\n",
    "\n",
    "    def _tokenization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Tokenization -- which is splitting the sentence into words.\n",
    "        \"\"\"\n",
    "        # 5 | tokenize.\n",
    "        def tokenize_sentence(sentence):\n",
    "            return nltk.word_tokenize(sentence)\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: tokenize_sentence(sentence))\n",
    "\n",
    "    def _remove_stop_words(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the stop words from the tokens.\n",
    "        \"\"\"\n",
    "        def remove_stop_word(words: list) -> list:\n",
    "            \"\"\"  \n",
    "            Remove the stop words from the list of words.\n",
    "\n",
    "            :param words: The list of words in a sentence.\n",
    "            :returns: List of words which are not stop words.\n",
    "            \"\"\"\n",
    "            stop_words = stopwords.words('english')\n",
    "            return [word for word in words if word not in stop_words]\n",
    "            \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda words: remove_stop_word(words))\n",
    "\n",
    "    def _lemmatization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Lemmatization -- which is converting every word into it's root form.\n",
    "        \"\"\"\n",
    "\n",
    "        def create_lemma(words: list):\n",
    "            \"\"\" \n",
    "            Create the lemma for the list of string.\n",
    "            \"\"\"\n",
    "            lemmetizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmetizer.lemmatize(token) for token in words]\n",
    "            return lemmatized_tokens\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda row: create_lemma(row))\n",
    "\n",
    "    def run(self, save_preprocessed_dataframe: bool = True,\n",
    "            verbose: bool = True,\n",
    "            fix_typo: bool = True,\n",
    "            lemmatize: bool = True,\n",
    "            remove_stop_words: bool = True,\n",
    "            ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Actually run the preprocessing pipeline.\n",
    "        Steps which are involved in it.\n",
    "        * 1 - Remove symbols (including numbers, symbols, special characters)\n",
    "        * 2 - Remove links.\n",
    "        * 3 - Case fold -- change all to lower case.\n",
    "        * 4 - Fix typos.\n",
    "        * 5 - tokenization.\n",
    "        * 6 - Remove stop words.\n",
    "        * 7 - lemmatization.\n",
    "        \"\"\"\n",
    "        star_print_length: int = 70\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "        if verbose: print(\"Preprocessing data...\")\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "\n",
    "        # 1 | Remove symbols (including numbers, symbols, special characters).\n",
    "        if verbose: print(\"\")\n",
    "        if verbose: print(\"\\tRemoving numbers, punctuations and special characters...\")\n",
    "        self._remove_symbols()\n",
    "\n",
    "        # 2 | Remove links.\n",
    "        if verbose: print(\"\\tRemoving links...\")\n",
    "        self._remove_links()\n",
    "\n",
    "        # 3 | Case fold -- change all to lower case.\n",
    "        if verbose: print(\"\\tChanging sentence to lower case...\")\n",
    "        self._case_fold_to_lower_case()\n",
    "\n",
    "        # 4 | Fix typos.\n",
    "        if fix_typo:\n",
    "            if verbose: print(\"\\tFixing typos...\")\n",
    "            self._fix_typo()\n",
    "\n",
    "        # 5 | tokenization.\n",
    "        if verbose: print(\"\\tTokenization -- Splitting the sentence into tokens...\")\n",
    "        self._tokenization()\n",
    "\n",
    "        # 6 | Remove stop words.\n",
    "        if remove_stop_words:\n",
    "            if verbose: print(\"\\tRemoving stop words...\")\n",
    "            self._remove_stop_words()\n",
    "\n",
    "        # 7 | lemmatization.\n",
    "        if lemmatize:\n",
    "            if verbose: print(\"\\tLemmatization...\")\n",
    "            self._lemmatization()\n",
    "\n",
    "        # save the dataframe, so that we can skip the preprocessing next time.\n",
    "        if save_preprocessed_dataframe:\n",
    "            if verbose: print(\"\\tSaving the dataframe for future use...\")\n",
    "            data.to_pickle(self.file_name)\n",
    "            # pd.read_pickle(file_name)\n",
    "\n",
    "        if verbose: print()\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "        if verbose: print(\"Preprocessing is done successfully.\")\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# can be done easily using spacy -- but less control over it\n",
    "# def spacy_tokenizer(doc):\n",
    "#   return [t.lemma_.lower() for t in nlp(doc) if \\\n",
    "#           len(t) > 2 and \\\n",
    "#           not t.is_punct and \\\n",
    "#           not t.is_space and \\\n",
    "#           not t.is_stop and \\\n",
    "#           t.is_alpha]\n",
    "# tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\", lower=False, split='|', oov_token='OOV')\n",
    "\n",
    "# # Import the NLP module which contains Spark NLP and NLU libraries\n",
    "# from johnsnowlabs import nlp\n",
    "# spark = nlp.start(nlp=False)\n",
    "\n",
    "# # Use Norvig model\n",
    "# nlp.load(\"en.spell.norvig\").predict(\"Plaese alliow me tao introdduce myhelf, I am a man of wealth und tiaste\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | word to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the source code:\n",
    "\n",
    "# - **fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency.\n",
    "#  So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1;\n",
    "# word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding.\n",
    "# So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "\n",
    "# - **texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and \n",
    "# replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "\n",
    "\n",
    "# Why don't combine them? Because you almost always fit once and convert to sequences many times.\n",
    "# You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction\n",
    "#time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.\n",
    "\n",
    "\n",
    "class WordToVectors:\n",
    "    \"\"\"\n",
    "    Converting the word to vectors(numbers). Using pretrained \"word2vec\" model. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the variables.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.column_name = column_name\n",
    "        self.tokenizer = None\n",
    "        # To store the generated vectors.\n",
    "        self.df = pd.DataFrame()\n",
    "        self.token_matrix = None\n",
    "\n",
    "        # Store the word-index for future reference.\n",
    "        self.word_index = None\n",
    "        self.embedding_matrix: np.ndarray = None\n",
    "\n",
    "        # get the inform about the embedding metrix.\n",
    "        self.num_tokens: int = 0\n",
    "        self.embedding_matrix_num_features = 0\n",
    "\n",
    "    def _tokens_to_vectors(self) -> None:\n",
    "        \"\"\" \n",
    "        Generate the vectors from the tokens.\n",
    "        \"\"\"\n",
    "        # First converting the tokens into corresponding numbers.\n",
    "        # oov_token='<OOV>' replaces all out-of-vocabulary words with <OOV>.\n",
    "        if not self.tokenizer:\n",
    "            self.tokenizer = keras.preprocessing.text.Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
    "\n",
    "        # give the tweets texts to the models and fit the texts.\n",
    "        self.tokenizer.fit_on_texts(self.data[self.column_name])\n",
    "\n",
    "        # why token_index -- because the number is actually the index of the word which is stored in the word_index dictionary.\n",
    "        self.token_matrix = self.tokenizer.texts_to_sequences(self.data[self.column_name])\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "\n",
    "    def _sequence_padding(self, max_padding_length: int) -> None:\n",
    "        \"\"\" \n",
    "        Pad the sequence, this is because all the sentence won't be in the same length.\n",
    "        We will decide the max padding length and convert all the sentence to that length.\n",
    "        if the sentence length < max_padding_length --> fill the remaining place with 0.\n",
    "        if the sentence length > max_padding_length --> Trim the sentence.\n",
    "        \"\"\"\n",
    "        self.token_matrix = keras.preprocessing.sequence.pad_sequences(self.token_matrix, \n",
    "                                                                       maxlen=max_padding_length)\n",
    "    \n",
    "    def _generate_n_grams(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Generate the n grams.\n",
    "        so the sentence now will be [ '1' ,'2' ,'3', '4' ]\n",
    "\n",
    "        Then, we have to make a n_gram model for good prediction.\n",
    "        this sequence will be splitted into multiple inputs as..\n",
    "\n",
    "                [ '1' ,'2' ,'3', '4' ] -- [\"My\" ,\"name\", \"is\", \"pavithra\"]\n",
    "\n",
    "                [ '1' ,'2' ,'3' ] -- [\"My\" ,\"name\", \"is\"]\n",
    "                \n",
    "                [ '1' ,'2' ] -- [\"My\" ,\"name\"]\n",
    "\n",
    "        :param data: The dataframe.\n",
    "        :param column_name: Where the tokens are present.\n",
    "        :returns: A dataframe with n gram sequence as column values.\n",
    "        \"\"\"\n",
    "        # store the thing in a dataframe.\n",
    "        df_n_grams_target: pd.DataFrame = pd.DataFrame({\"target\": []})\n",
    "        array: list = []\n",
    "        target = []\n",
    "\n",
    "        for row in self.token_matrix:\n",
    "            for index in range(2, len(row) + 1):\n",
    "                array.append(row[:index][:-1])\n",
    "                target.append(row[:index][-1])\n",
    "\n",
    "        # after creating the n-grams split it into features and labels.\n",
    "        del self.df\n",
    "        self.df = pd.DataFrame()\n",
    "        self.token_matrix = array\n",
    "        df_n_grams_target[\"target\"] = target\n",
    "\n",
    "        print(f\"\\tNumber of total sequences --> {len(self.token_matrix)}\")\n",
    "        \n",
    "        return df_n_grams_target\n",
    "    \n",
    "    def _create_embedding_matrix(self) -> None:\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # load the google pre-build model.\n",
    "        word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/home/pavithra/projects/NLP/GoogleNews-vectors-negative300.bin', binary=True,)\n",
    "\n",
    "        # + 1 to account for padding token. 0 is always reserved for padding in enbedding layer.\n",
    "        self.num_tokens: int = len(self.word_index) + 1\n",
    "\n",
    "        # Initialize a matrix of zeroes of size: vocabulary * embedding dimension.\n",
    "        self.embedding_matrix_num_features: int = 300\n",
    "        self.embedding_matrix = np.zeros((self.num_tokens, self.embedding_matrix_num_features))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            if word_vectors.has_index_for(word):\n",
    "                self.embedding_matrix[i] = word_vectors[word].copy()\n",
    "    \n",
    "    def run(self, max_padding_length: int):\n",
    "        \"\"\" \n",
    "        Convert the tokens into vectors.\n",
    "\n",
    "        :returns df: The dataframe has the tokens list.\n",
    "        :returns target: The target dataframe.\n",
    "        :returns num_tokens: Number of tokens in the vocabulary.\n",
    "        :returns embedding_matrix_num_features: the dimention of the embedding matrix.\n",
    "        :returns embedding_matrix: The embedding matrix.\n",
    "        \"\"\"\n",
    "        star_print_length: int = 70\n",
    "        print()\n",
    "        print(\"*\"*star_print_length)\n",
    "        print(\"Word to vectorization process in progress...\")\n",
    "        print(\"*\"*star_print_length)\n",
    "\n",
    "        print(\"\\tToken to numbers ...\")\n",
    "        self._tokens_to_vectors()  \n",
    "\n",
    "        # for this models i wanna do n-gram before padding the sequence .\n",
    "        print(\"\\tGenerating the n-grams ...\")\n",
    "        target = self._generate_n_grams()\n",
    "\n",
    "        print(\"\\tpadding the sequence tokens...\")\n",
    "        self._sequence_padding(max_padding_length)\n",
    "\n",
    "        print(\"\\tCreating embedding matrix...\")\n",
    "        self._create_embedding_matrix()\n",
    "        \n",
    "        print(\"\\tSaving the embedding matrix...\")\n",
    "        file_name: str = \"embedding_matrix\" # the extension will be npy\n",
    "        np.save(file_name, self.embedding_matrix, allow_pickle=True)\n",
    "        # to load -- np.load(file_name, allow_pickle=True)\n",
    "\n",
    "        print(f\"Number of tokens in the vocabulary --> {self.num_tokens - 1}\")\n",
    "\n",
    "        print()\n",
    "        print(\"*\"*star_print_length)\n",
    "        print(\"Converted words to vectors.\")\n",
    "        print(\"*\"*star_print_length)\n",
    "\n",
    "        return self.token_matrix, target, self.num_tokens, self.embedding_matrix_num_features, self.embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 | Generate n-grams and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have sentence in each word represented by numbers, we have to create a n-gram data.\n",
    "we have sentence like \"My name is Pavithra\" --> this will get converted into a sequence with their respective tokens.\n",
    "\n",
    "word_index dictionary for it:\n",
    "    {\n",
    "        \"My\": 1,\n",
    "        \"name\": 2,\n",
    "        \"is\": 3,\n",
    "        \"Pavithra\": 4\n",
    "    }\n",
    "\n",
    "so the sentence now will be [ '1' ,'2' ,'3', '4' ]\n",
    "\n",
    "Then, we have to make a n_gram model for good prediction.\n",
    "this sequence will be splitted into multiple inputs as..\n",
    "\n",
    " [ '1' ,'2' ,'3', '4' ] -- [\"My\" ,\"name\", \"is\", \"pavithra\"]\n",
    "\n",
    " [ '1' ,'2' ,'3' ] -- [\"My\" ,\"name\", \"is\"]\n",
    " \n",
    " [ '1' ,'2' ] -- [\"My\" ,\"name\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data: pd.DataFrame, column_name: str, total_unique_words: int) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    One hot encode the dataframe values.\n",
    "\n",
    "    :returns: one-hot encoded dataframe.\n",
    "    \"\"\"\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    # NOTE: num_classes -- the max value you will be having in the list + 1. since we need to create that many columns.\n",
    "    # ex: \n",
    "    # [1,3,5] -- output will be having 6 columns --> [0,1,2,3,4,5] {index will always start from 0 in python} -- total classes here is 6.\n",
    "    # \n",
    "    one_hot_matrix = to_categorical(data[column_name], num_classes=total_unique_words)\n",
    "    one_hot_df = pd.DataFrame(one_hot_matrix)\n",
    "\n",
    "    return one_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 | Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, num_tokens, embedding_dim, embedding_matrix, padding_length,\n",
    "                weight_decay=1e-4, dropout_rate=0.2, lr=0.001,\n",
    "                num_epoches=20, batch_size=256):\n",
    "    \"\"\" \n",
    "    Create the LSTM model and return the model object.\n",
    "\n",
    "    :param features: tuple of 4 values -- X_train, y_train, X-val, y_val.\n",
    "    :param num_tokens: The number of tokens in the vocabulary.\n",
    "    :param embedding_dim: The dimention of the embedding matrix.\n",
    "    :param padding_length: the length used for padding, this is the input feature length.\n",
    "    :param weight_decay: Delay for the l2 regularization. Default is:1e-4.\n",
    "    :param dropout_rate: Dropout regularization rate.\n",
    "    :param lr: learning rate.\n",
    "    :param num_epoches: The number of epoches,\n",
    "    :param batch_size: batch size to consider for each gradient decent.\n",
    "\n",
    "    :returns:  the DL model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add the embedding layer as the first layer.\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "                            num_tokens,\n",
    "                            output_dim=100,\n",
    "                            # embedding_dim,\n",
    "                            #embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=padding_length,\n",
    "                            trainable=True\n",
    "    )\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    # add a lstm layer and dropout layer to prevent overfittitng.\n",
    "    # model.add(keras.layers.LSTM(units=250, return_sequences=True, \n",
    "    #                             kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    # model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # add a lstm layer and dropout layer to prevent overfittitng.\n",
    "    model.add(keras.layers.Bidirectional(keras.layers.LSTM(350, kernel_regularizer=keras.regularizers.l2(weight_decay))))\n",
    "    # model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(250, activation='relu',\n",
    "                                 kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    \n",
    "    model.add(keras.layers.Dense(num_tokens, activation='softmax'))\n",
    "\n",
    "\n",
    "    # add the optimizers.\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    # get the callbacks.\n",
    "    # checkpoint_path = \"training_wights\"\n",
    "    # # Create a callback that saves the model's weights\n",
    "    # checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "    #                                                 save_weights_only=True,\n",
    "    #                                                 verbose=0,\n",
    "    #                                                 save_freq='epoch')\n",
    "    # patience is the number of epochs to wait before stopping, if the model is not improving.\n",
    "    early_stopping_cb = EarlyStopping(monitor='val_accuracy', verbose=0, patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # compile the model.\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model.\n",
    "    X_train, y_train, X_val, y_val = features\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=True,\n",
    "                        epochs=num_epoches,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping_cb])\n",
    "    \n",
    "\n",
    "    # save the model.\n",
    "    model_name: str = f\"saved_model/lstm_lr{lr}_batch_size_{batch_size}.keras\"\n",
    "    model.save(model_name)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the max sentence length\n",
    "\n",
    "# df_length = pd.DataFrame()\n",
    "# df_length[\"length\"] = data['title'].apply(lambda x: len(x))\n",
    "# max_length = df_length[\"length\"].max()\n",
    "\n",
    "# print(\"max length of the tweets --->\", max_length)\n",
    "# del df_length\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "# output\n",
    "#-----------------------------------\n",
    "# max length of the tweets ---> 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "Preprocessing data...\n",
      "**********************************************************************\n",
      "\n",
      "\tRemoving numbers, punctuations and special characters...\n",
      "\tRemoving links...\n",
      "\tChanging sentence to lower case...\n",
      "\tFixing typos...\n",
      "\tTokenization -- Splitting the sentence into tokens...\n",
      "\tSaving the dataframe for future use...\n",
      "\n",
      "**********************************************************************\n",
      "Preprocessing is done successfully.\n",
      "**********************************************************************\n",
      "\n",
      "**********************************************************************\n",
      "Word to vectorization process in progress...\n",
      "**********************************************************************\n",
      "\tToken to numbers ...\n",
      "\tGenerating the n-grams ...\n",
      "\tNumber of total sequences --> 47864\n",
      "\tpadding the sequence tokens...\n",
      "\tCreating embedding matrix...\n",
      "\tSaving the embedding matrix...\n",
      "Number of tokens in the vocabulary --> 7717\n",
      "\n",
      "**********************************************************************\n",
      "Converted words to vectors.\n",
      "**********************************************************************\n",
      "\n",
      "Number of values in train X -----------> 40684\n",
      "Number of values in train y -----------> 40684\n",
      "Number of values in Val X -----------> 7180\n",
      "Number of values in val y -----------> 7180\n"
     ]
    }
   ],
   "source": [
    "COLUMN_NAME: Final[str] = \"title\"\n",
    "MAX_PADDING_LENGTH: Final[int] = 40\n",
    "TOTAL_WORDS_IN_VOC: int = 0\n",
    "\n",
    "data: pd.DataFrame = PreprocessData(data, COLUMN_NAME).run(remove_stop_words=False, lemmatize=False, ) # because i need the words like -- \"we, i, ...\"\n",
    "token_matrix, target_df , num_tokens, embedding_matrix_num_features, embedding_matrix = WordToVectors(data, COLUMN_NAME).run(MAX_PADDING_LENGTH)\n",
    "\n",
    "# convert the target into one-hot encoded value.\n",
    "TOTAL_WORDS_IN_VOC = num_tokens  # one extra was added for padding.\n",
    "target_df = one_hot_encode(target_df, \"target\", total_unique_words=TOTAL_WORDS_IN_VOC)\n",
    "\n",
    "# split the input into training and validation set.\n",
    "data = pd.DataFrame(token_matrix)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target_df, train_size=0.85, shuffle=True, random_state=1)\n",
    "\n",
    "# train the model.\n",
    "print(f\"\\nNumber of values in train X -----------> {len(X_train)}\")\n",
    "print(f\"Number of values in train y -----------> {len(y_train)}\")\n",
    "print(f\"Number of values in Val X -----------> {len(X_val)}\")\n",
    "print(f\"Number of values in val y -----------> {len(y_val)}\")\n",
    "\n",
    "# model, history = train_model((X_train, y_train, X_val, y_val), num_tokens=TOTAL_WORDS_IN_VOC,\n",
    "#                               embedding_dim=embedding_matrix_num_features,\n",
    "#                               embedding_matrix=embedding_matrix, \n",
    "#                               padding_length=MAX_PADDING_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    (None, 40, 100)           771800    \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 700)               1262800   \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 250)               175250    \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 7718)              1937218   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4147068 (15.82 MB)\n",
      "Trainable params: 4147068 (15.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "159/159 [==============================] - 97s 590ms/step - loss: 7.3254 - accuracy: 0.0530 - val_loss: 6.8535 - val_accuracy: 0.0974\n",
      "Epoch 2/20\n",
      "159/159 [==============================] - 110s 691ms/step - loss: 6.4363 - accuracy: 0.1160 - val_loss: 6.6805 - val_accuracy: 0.1252\n",
      "Epoch 3/20\n",
      "159/159 [==============================] - 126s 791ms/step - loss: 6.1153 - accuracy: 0.1379 - val_loss: 6.6923 - val_accuracy: 0.1421\n",
      "Epoch 4/20\n",
      "159/159 [==============================] - 130s 821ms/step - loss: 5.8722 - accuracy: 0.1564 - val_loss: 6.7540 - val_accuracy: 0.1425\n",
      "Epoch 5/20\n",
      "159/159 [==============================] - 125s 783ms/step - loss: 5.6830 - accuracy: 0.1698 - val_loss: 6.8861 - val_accuracy: 0.1494\n",
      "Epoch 6/20\n",
      "159/159 [==============================] - 118s 741ms/step - loss: 5.6110 - accuracy: 0.1781 - val_loss: 7.0257 - val_accuracy: 0.1513\n",
      "Epoch 7/20\n",
      "159/159 [==============================] - 118s 745ms/step - loss: 5.4140 - accuracy: 0.1878 - val_loss: 7.1590 - val_accuracy: 0.1547\n",
      "Epoch 8/20\n",
      "159/159 [==============================] - 113s 711ms/step - loss: 5.2898 - accuracy: 0.1965 - val_loss: 7.3572 - val_accuracy: 0.1582\n",
      "Epoch 9/20\n",
      "159/159 [==============================] - 110s 694ms/step - loss: 5.1913 - accuracy: 0.2044 - val_loss: 7.3782 - val_accuracy: 0.1603\n",
      "Epoch 10/20\n",
      "159/159 [==============================] - 102s 643ms/step - loss: 5.0926 - accuracy: 0.2087 - val_loss: 7.6463 - val_accuracy: 0.1664\n",
      "Epoch 11/20\n",
      "159/159 [==============================] - 105s 661ms/step - loss: 5.0134 - accuracy: 0.2143 - val_loss: 7.8319 - val_accuracy: 0.1623\n",
      "Epoch 12/20\n",
      "159/159 [==============================] - 105s 658ms/step - loss: 4.9318 - accuracy: 0.2203 - val_loss: 7.9225 - val_accuracy: 0.1648\n",
      "Epoch 13/20\n",
      "159/159 [==============================] - 105s 658ms/step - loss: 4.8587 - accuracy: 0.2244 - val_loss: 8.1437 - val_accuracy: 0.1652\n"
     ]
    }
   ],
   "source": [
    "model, history = train_model((X_train, y_train, X_val, y_val), num_tokens=TOTAL_WORDS_IN_VOC,\n",
    "                              num_epoches=20, lr=0.02,\n",
    "                              embedding_dim=embedding_matrix_num_features,\n",
    "                              embedding_matrix=embedding_matrix, \n",
    "                              padding_length=MAX_PADDING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pydot' has no attribute 'InvocationException'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pydot/core.py:1753\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1752\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1753\u001b[0m     stdout_data, stderr_data, process \u001b[39m=\u001b[39m call_graphviz(\n\u001b[1;32m   1754\u001b[0m         program\u001b[39m=\u001b[39;49mprog,\n\u001b[1;32m   1755\u001b[0m         arguments\u001b[39m=\u001b[39;49marguments,\n\u001b[1;32m   1756\u001b[0m         working_dir\u001b[39m=\u001b[39;49mtmp_dir,\n\u001b[1;32m   1757\u001b[0m     )\n\u001b[1;32m   1758\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pydot/core.py:133\u001b[0m, in \u001b[0;36mcall_graphviz\u001b[0;34m(program, arguments, working_dir, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m program_with_args \u001b[39m=\u001b[39m [program] \u001b[39m+\u001b[39m arguments\n\u001b[0;32m--> 133\u001b[0m process \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39;49mPopen(\n\u001b[1;32m    134\u001b[0m     program_with_args,\n\u001b[1;32m    135\u001b[0m     env\u001b[39m=\u001b[39;49menv,\n\u001b[1;32m    136\u001b[0m     cwd\u001b[39m=\u001b[39;49mworking_dir,\n\u001b[1;32m    137\u001b[0m     shell\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    138\u001b[0m     stderr\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m    139\u001b[0m     stdout\u001b[39m=\u001b[39;49msubprocess\u001b[39m.\u001b[39;49mPIPE,\n\u001b[1;32m    140\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    141\u001b[0m )\n\u001b[1;32m    142\u001b[0m stdout_data, stderr_data \u001b[39m=\u001b[39m process\u001b[39m.\u001b[39mcommunicate()\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:858\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mTextIOWrapper(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstderr,\n\u001b[1;32m    856\u001b[0m                     encoding\u001b[39m=\u001b[39mencoding, errors\u001b[39m=\u001b[39merrors)\n\u001b[0;32m--> 858\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_execute_child(args, executable, preexec_fn, close_fds,\n\u001b[1;32m    859\u001b[0m                         pass_fds, cwd, env,\n\u001b[1;32m    860\u001b[0m                         startupinfo, creationflags, shell,\n\u001b[1;32m    861\u001b[0m                         p2cread, p2cwrite,\n\u001b[1;32m    862\u001b[0m                         c2pread, c2pwrite,\n\u001b[1;32m    863\u001b[0m                         errread, errwrite,\n\u001b[1;32m    864\u001b[0m                         restore_signals, start_new_session)\n\u001b[1;32m    865\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[39m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/subprocess.py:1704\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1703\u001b[0m         err_msg \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1704\u001b[0m     \u001b[39mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1705\u001b[0m \u001b[39mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dot'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/vis_utils.py:57\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Attempt to create an image of a blank graph\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39m# to check the pydot/graphviz installation.\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     pydot\u001b[39m.\u001b[39;49mDot\u001b[39m.\u001b[39;49mcreate(pydot\u001b[39m.\u001b[39;49mDot())\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pydot/core.py:1762\u001b[0m, in \u001b[0;36mDot.create\u001b[0;34m(self, prog, format, encoding)\u001b[0m\n\u001b[1;32m   1761\u001b[0m     args[\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{prog}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m not found in path.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(prog\u001b[39m=\u001b[39mprog)\n\u001b[0;32m-> 1762\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1763\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] \"dot\" not found in path.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/pavithra/projects/Natural-Language-Processing/project_2_next_word_predictor/main.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pavithra/projects/Natural-Language-Processing/project_2_next_word_predictor/main.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m plot_model\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pavithra/projects/Natural-Language-Processing/project_2_next_word_predictor/main.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m plot_model(model, to_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodel_plot.png\u001b[39;49m\u001b[39m'\u001b[39;49m, show_shapes\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, show_layer_names\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/vis_utils.py:451\u001b[0m, in \u001b[0;36mplot_model\u001b[0;34m(model, to_file, show_shapes, show_dtype, show_layer_names, rankdir, expand_nested, dpi, layer_range, show_layer_activations, show_trainable)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39mbuilt:\n\u001b[1;32m    445\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis model has not yet been built. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mBuild the model first by calling `build()` or by calling \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe model on a batch of data.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     )\n\u001b[0;32m--> 451\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m check_graphviz():\n\u001b[1;32m    452\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    453\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou must install pydot (`pip install pydot`) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand install graphviz \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    455\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(see instructions at https://graphviz.gitlab.io/download/) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    456\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor plot_model to work.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    457\u001b[0m     )\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mIPython.core.magics.namespace\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m    459\u001b[0m         \u001b[39m# We don't raise an exception here in order to avoid crashing\u001b[39;00m\n\u001b[1;32m    460\u001b[0m         \u001b[39m# notebook tests where graphviz is not available.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/src/utils/vis_utils.py:59\u001b[0m, in \u001b[0;36mcheck_graphviz\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m     pydot\u001b[39m.\u001b[39mDot\u001b[39m.\u001b[39mcreate(pydot\u001b[39m.\u001b[39mDot())\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, pydot\u001b[39m.\u001b[39;49mInvocationException):\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pydot' has no attribute 'InvocationException'"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
