{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is about predicting the next word of the sentence, this can be told mathematical as\n",
    "\n",
    "P(wi+1 | wi, wi-1, wi-2) => predicting the word i+1 based on the words i, i-1, i-2 ...\n",
    "\n",
    "Real life examples:\n",
    "- In google search bar when you type some words it will show the completion\n",
    "- Whatsapp or other places when you typing, it will show the next word suggestion\n",
    "- Gmail word completion while writing the mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-03 19:36:23.747421: W tensorflow/stream_executor/platform/default/dso_loader.cc:59] Could not load dynamic library 'libcudart.so.10.1'; dlerror: libcudart.so.10.1: cannot open shared object file: No such file or directory\n",
      "2024-05-03 19:36:23.747443: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# basic packages.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import kaggle_utils as utils \n",
    "\n",
    "# visualization packages.\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# NLP framework.\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "import gensim\n",
    "\n",
    "# model preprocessing and metrics.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================= Common data analysis =======================================\n",
      "\n",
      "Numerical data list [] ---> total 0 numerical values\n",
      "Categorical data list ['title'] ---> total 1 categorical values\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d46a3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d46a3_level0_col0\" class=\"col_heading level0 col0\" >data type</th>\n",
       "      <th id=\"T_d46a3_level0_col1\" class=\"col_heading level0 col1\" >Missing Value(NA)</th>\n",
       "      <th id=\"T_d46a3_level0_col2\" class=\"col_heading level0 col2\" >?[]na null ' ' </th>\n",
       "      <th id=\"T_d46a3_level0_col3\" class=\"col_heading level0 col3\" >% of Missing value(NA)</th>\n",
       "      <th id=\"T_d46a3_level0_col4\" class=\"col_heading level0 col4\" >% of Missing value(?[]na null ' ')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d46a3_level0_row0\" class=\"row_heading level0 row0\" >title</th>\n",
       "      <td id=\"T_d46a3_row0_col0\" class=\"data row0 col0\" >object</td>\n",
       "      <td id=\"T_d46a3_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_d46a3_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_d46a3_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_d46a3_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f5ac96936d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Get the data\n",
    "file_path: str = \"data/medium_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head(5)\n",
    "\n",
    "# We are just gonna consider the title of the articles.\n",
    "data = data[[\"title\"]]\n",
    "\n",
    "# Basic data common analysis.\n",
    "column, categorical_data, numerical_data, missing_data = utils.Common_data_analysis(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "\n",
    "\n",
    "# can i remove punctuation: yes i have to\n",
    "# can i remove links: yes it is not gonna give any useful information here\n",
    "# can i remove numbers: yes, we can because \"2 people died through the disaster\" and  \"people died through the disaster\" are both gonna give the same meaning.\n",
    "# can i remove stop words: yes, we can.\n",
    "\n",
    "\n",
    "class PreprocessData:\n",
    "    \"\"\"\n",
    "    Class to preprocess the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the dataframe.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.column_name: str = column_name\n",
    "        self.file_name: str = \"pre-processed-data.pkl\"\n",
    "\n",
    "    def _remove_symbols(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the numbers.\n",
    "        * Removes punctuations.\n",
    "        * Removes any symbols.\n",
    "        \"\"\"\n",
    "        # 1 | remove the punctuation words and symbols and numbers.\n",
    "        # will create a regex and apply over the text.\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"[0123456789!\\\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", sentence)) \n",
    "\n",
    "    def _remove_links(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes any links.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"(w+://S+)\", \" \", sentence))\n",
    "\n",
    "    def _case_fold_to_lower_case(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Change the sentence into lower case.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].str.lower()\n",
    "\n",
    "    def _fix_typo(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Fix the typo of the each word -- in real word when you get data from tweets, mails, articles etc\n",
    "        * there is a chance of typo since they are written by humans. We need to fix it to get better results.\n",
    "        \"\"\"\n",
    "        # we are using \n",
    "        pass\n",
    "\n",
    "    def _tokenization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Tokenization -- which is splitting the sentence into words.\n",
    "        \"\"\"\n",
    "        # 5 | tokenize.\n",
    "        def tokenize_sentence(sentence):\n",
    "            return nltk.word_tokenize(sentence)\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: tokenize_sentence(sentence))\n",
    "\n",
    "    def _remove_stop_words(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the stop words from the tokens.\n",
    "        \"\"\"\n",
    "        def remove_stop_word(words: list) -> list:\n",
    "            \"\"\"  \n",
    "            Remove the stop words from the list of words.\n",
    "\n",
    "            :param words: The list of words in a sentence.\n",
    "            :returns: List of words which are not stop words.\n",
    "            \"\"\"\n",
    "            stop_words = stopwords.words('english')\n",
    "            return [word for word in words if word not in stop_words]\n",
    "            \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda words: remove_stop_word(words))\n",
    "\n",
    "    def _lemmatization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Lemmatization -- which is converting every word into it's root form.\n",
    "        \"\"\"\n",
    "\n",
    "        def create_lemma(words: list):\n",
    "            \"\"\" \n",
    "            Create the lemma for the list of string.\n",
    "            \"\"\"\n",
    "            lemmetizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmetizer.lemmatize(token) for token in words]\n",
    "            return lemmatized_tokens\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda row: create_lemma(row))\n",
    "\n",
    "    def run(self, save_preprocessed_dataframe: bool = True,\n",
    "            verbose: bool = True,\n",
    "            fix_typo: bool = True,\n",
    "            lemmatize: bool = True,\n",
    "            remove_stop_words: bool = True,\n",
    "            ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Actually run the preprocessing pipeline.\n",
    "        Steps which are involved in it.\n",
    "        * 1 - Remove symbols (including numbers, symbols, special characters)\n",
    "        * 2 - Remove links.\n",
    "        * 3 - Case fold -- change all to lower case.\n",
    "        * 4 - Fix typos.\n",
    "        * 5 - tokenization.\n",
    "        * 6 - Remove stop words.\n",
    "        * 7 - lemmatization.\n",
    "        \"\"\"\n",
    "        if verbose: print(\"*\"*50)\n",
    "        if verbose: print(\"\\nPreprocessing data...\")\n",
    "        if verbose: print(\"*\"*50)\n",
    "\n",
    "        # 1 | Remove symbols (including numbers, symbols, special characters).\n",
    "        if verbose: print(\"\")\n",
    "        if verbose: print(\"\\tRemoving numbers, punctuations and special characters...\")\n",
    "        self._remove_symbols()\n",
    "\n",
    "        # 2 | Remove links.\n",
    "        \n",
    "        self._remove_links()\n",
    "        if verbose: print(\"\\tRemoving links...\")\n",
    "        # 3 | Case fold -- change all to lower case.\n",
    "        self._case_fold_to_lower_case()\n",
    "\n",
    "        # 4 | Fix typos.\n",
    "        if fix_typo:\n",
    "            if verbose: print(\"\\tFixing typos...\")\n",
    "            self._fix_typo()\n",
    "\n",
    "        # 5 | tokenization.\n",
    "        if verbose: print(\"\\tTokenization -- Splitting the sentence into tokens...\")\n",
    "        self._tokenization()\n",
    "\n",
    "        # 6 | Remove stop words.\n",
    "        if remove_stop_words:\n",
    "            if verbose: print(\"\\tRemoving stop words...\")\n",
    "            self._remove_stop_words()\n",
    "\n",
    "        # 7 | lemmatization.\n",
    "        if lemmatize:\n",
    "            if verbose: print(\"\\tLemmatization...\")\n",
    "            self._lemmatization()\n",
    "\n",
    "        # save the dataframe, so that we can skip the preprocessing next time.\n",
    "        if save_preprocessed_dataframe:\n",
    "            if verbose: print(\"\\tSaving the dataframe for future use...\")\n",
    "            data.to_pickle(self.file_name)\n",
    "            # pd.read_pickle(file_name)\n",
    "\n",
    "        if verbose: print(\"Preprocessing is done successfully.\")\n",
    "        if verbose: print(\"*\"*50)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# can be done easily using spacy -- but less control over it\n",
    "# def spacy_tokenizer(doc):\n",
    "#   return [t.lemma_.lower() for t in nlp(doc) if \\\n",
    "#           len(t) > 2 and \\\n",
    "#           not t.is_punct and \\\n",
    "#           not t.is_space and \\\n",
    "#           not t.is_stop and \\\n",
    "#           t.is_alpha]\n",
    "# tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\", lower=False, split='|', oov_token='OOV')\n",
    "\n",
    "# # Import the NLP module which contains Spark NLP and NLU libraries\n",
    "# from johnsnowlabs import nlp\n",
    "# spark = nlp.start(nlp=False)\n",
    "\n",
    "# # Use Norvig model\n",
    "# nlp.load(\"en.spell.norvig\").predict(\"Plaese alliow me tao introdduce myhelf, I am a man of wealth und tiaste\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the source code:\n",
    "\n",
    "# - **fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency.\n",
    "#  So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1;\n",
    "# word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding.\n",
    "# So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "\n",
    "# - **texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and \n",
    "# replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "\n",
    "\n",
    "# Why don't combine them? Because you almost always fit once and convert to sequences many times.\n",
    "# You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction\n",
    "#time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.\n",
    "\n",
    "\n",
    "class WordToVectors:\n",
    "    \"\"\"\n",
    "    Converting the word to vectors(numbers). Using pretrained \"word2vec\" model. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the variables.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.column_name = column_name\n",
    "        self.tokenizer = None\n",
    "        # To store the generated vectors.\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "        # Store the word-index for future reference.\n",
    "        self.word_index = None\n",
    "        self.embedding_matrix = None\n",
    "\n",
    "        # get the inform about the embedding metrix.\n",
    "        self.num_tokens: int = 0\n",
    "        self.embedding_matrix_num_features = 0\n",
    "\n",
    "    def _tokens_to_vectors(self) -> None:\n",
    "        \"\"\" \n",
    "        Generate the vectors from the tokens.\n",
    "        \"\"\"\n",
    "        # First converting the tokens into corresponding numbers.\n",
    "        # oov_token='<OOV>' replaces all out-of-vocabulary words with <OOV>.\n",
    "        if not self.tokenizer:\n",
    "            self.tokenizer = keras.preprocessing.text.Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
    "\n",
    "        # give the tweets texts to the models and fit the texts.\n",
    "        self.tokenizer.fit_on_texts(self.data[self.column_name])\n",
    "\n",
    "        # why token_index -- because the number is actually the index of the word which is stored in the word_index dictionary.\n",
    "        self.df[\"token_index\"] = self.tokenizer.texts_to_sequences(self.data[self.column_name])\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "\n",
    "    def _sequence_padding(self, max_padding_length: int) -> None:\n",
    "        \"\"\" \n",
    "        Pad the sequence, this is because all the sentence won't be in the same length.\n",
    "        We will decide the max padding length and convert all the sentence to that length.\n",
    "        if the sentence length < max_padding_length --> fill the remaining place with 0.\n",
    "        if the sentence length > max_padding_length --> Trim the sentence.\n",
    "        \"\"\"\n",
    "        self.df[\"token_index\"] = keras.preprocessing.sequence.pad_sequences(self.data, \n",
    "                                                                            maxlen=max_padding_length)\n",
    "    \n",
    "    def _create_embedding_matrix(self) -> None:\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # load the google pre-build model.\n",
    "        word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/home/pavithra/projects/NLP/GoogleNews-vectors-negative300.bin', binary=True,)\n",
    "\n",
    "        # + 1 to account for padding token. 0 is always reserved for padding in enbedding layer.\n",
    "        self.num_tokens: int = len(self.word_index) + 1\n",
    "\n",
    "        # Initialize a matrix of zeroes of size: vocabulary * embedding dimension.\n",
    "        self.embedding_matrix_num_features: int = 300\n",
    "        self.embedding_matrix = np.zeros((self.num_tokens, self.embedding_matrix_num_features))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            if word_vectors.has_index_for(word):\n",
    "                self.embedding_matrix[i] = word_vectors[word].copy()\n",
    "    \n",
    "    def run(self, max_padding_length: int):\n",
    "        \"\"\" \n",
    "        Convert the tokens into vectors.\n",
    "        \"\"\"\n",
    "        print(\"Word to vectorization process in progress...\")\n",
    "        self._tokens_to_vectors()\n",
    "        self._sequence_padding(max_padding_length)\n",
    "        self._create_embedding_matrix()\n",
    "\n",
    "        return self.num_tokens, self.embedding_matrix_num_features, self.embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
