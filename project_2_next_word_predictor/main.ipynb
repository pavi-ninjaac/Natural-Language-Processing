{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "This is about predicting the next word of the sentence, this can be told mathematical as\n",
    "\n",
    "P(wi+1 | wi, wi-1, wi-2) => predicting the word i+1 based on the words i, i-1, i-2 ...\n",
    "\n",
    "Real life examples:\n",
    "- In google search bar when you type some words it will show the completion\n",
    "- Whatsapp or other places when you typing, it will show the next word suggestion\n",
    "- Gmail word completion while writing the mail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages.\n",
    "from typing import Final\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import kaggle_utils as utils \n",
    "\n",
    "# visualization packages.\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# NLP framework.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow import keras\n",
    "import gensim\n",
    "\n",
    "# model preprocessing and metrics.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================= Common data analysis =======================================\n",
      "\n",
      "Numerical data list [] ---> total 0 numerical values\n",
      "Categorical data list ['title'] ---> total 1 categorical values\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_d52ba\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d52ba_level0_col0\" class=\"col_heading level0 col0\" >data type</th>\n",
       "      <th id=\"T_d52ba_level0_col1\" class=\"col_heading level0 col1\" >Missing Value(NA)</th>\n",
       "      <th id=\"T_d52ba_level0_col2\" class=\"col_heading level0 col2\" >?[]na null ' ' </th>\n",
       "      <th id=\"T_d52ba_level0_col3\" class=\"col_heading level0 col3\" >% of Missing value(NA)</th>\n",
       "      <th id=\"T_d52ba_level0_col4\" class=\"col_heading level0 col4\" >% of Missing value(?[]na null ' ')</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d52ba_level0_row0\" class=\"row_heading level0 row0\" >title</th>\n",
       "      <td id=\"T_d52ba_row0_col0\" class=\"data row0 col0\" >object</td>\n",
       "      <td id=\"T_d52ba_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_d52ba_row0_col2\" class=\"data row0 col2\" >0</td>\n",
       "      <td id=\"T_d52ba_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_d52ba_row0_col4\" class=\"data row0 col4\" >0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f35be791940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Beginner’s Guide to Word Embedding with Gens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hands-on Graph Neural Networks with PyTorch &amp; ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How to Use ggplot2 in Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Databricks: How to Save Files in CSV on Your L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Step-by-Step Implementation of Gradient Desc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6503</th>\n",
       "      <td>“We” vs “I” — How Should You Talk About Yourse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6504</th>\n",
       "      <td>How Donald Trump Markets Himself</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6505</th>\n",
       "      <td>Content and Marketing Beyond Mass Consumption</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6506</th>\n",
       "      <td>5 Questions All Copywriters Should Ask Clients...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6507</th>\n",
       "      <td>How To Write a Good Business Blog Post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6508 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  title\n",
       "0     A Beginner’s Guide to Word Embedding with Gens...\n",
       "1     Hands-on Graph Neural Networks with PyTorch & ...\n",
       "2                          How to Use ggplot2 in Python\n",
       "3     Databricks: How to Save Files in CSV on Your L...\n",
       "4     A Step-by-Step Implementation of Gradient Desc...\n",
       "...                                                 ...\n",
       "6503  “We” vs “I” — How Should You Talk About Yourse...\n",
       "6504                   How Donald Trump Markets Himself\n",
       "6505      Content and Marketing Beyond Mass Consumption\n",
       "6506  5 Questions All Copywriters Should Ask Clients...\n",
       "6507             How To Write a Good Business Blog Post\n",
       "\n",
       "[6508 rows x 1 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the data\n",
    "file_path: str = \"data/medium_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "data.head(5)\n",
    "\n",
    "# We are just gonna consider the title of the articles.\n",
    "data = data[[\"title\"]]\n",
    "\n",
    "# Basic data common analysis.\n",
    "column, categorical_data, numerical_data, missing_data = utils.Common_data_analysis(data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing \n",
    "\n",
    "\n",
    "# can i remove punctuation: yes i have to\n",
    "# can i remove links: yes it is not gonna give any useful information here\n",
    "# can i remove numbers: yes, we can because \"2 people died through the disaster\" and  \"people died through the disaster\" are both gonna give the same meaning.\n",
    "# can i remove stop words: yes, we can.\n",
    "\n",
    "\n",
    "class PreprocessData:\n",
    "    \"\"\"\n",
    "    Class to preprocess the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the dataframe.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data: pd.DataFrame = data\n",
    "        self.column_name: str = column_name\n",
    "        self.file_name: str = \"pre-processed-data.pkl\"\n",
    "\n",
    "    def _remove_symbols(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the numbers.\n",
    "        * Removes punctuations.\n",
    "        * Removes any symbols.\n",
    "        \"\"\"\n",
    "        # 1 | remove the punctuation words and symbols and numbers.\n",
    "        # will create a regex and apply over the text.\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"[0123456789!\\\"\\'\\’\\“\\”\\—#$%&()*+,-./:;<=>?@[\\]^_`{|}~]\", \" \", sentence)) \n",
    "\n",
    "    def _remove_links(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes any links.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: re.sub(r\"(w+://S+)\", \" \", sentence))\n",
    "\n",
    "    def _case_fold_to_lower_case(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Change the sentence into lower case.\n",
    "        \"\"\"\n",
    "        self.data[self.column_name] = self.data[self.column_name].str.lower()\n",
    "\n",
    "    def _fix_typo(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Fix the typo of the each word -- in real word when you get data from tweets, mails, articles etc\n",
    "        * there is a chance of typo since they are written by humans. We need to fix it to get better results.\n",
    "        \"\"\"\n",
    "        # we are using \n",
    "        pass\n",
    "\n",
    "    def _tokenization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Tokenization -- which is splitting the sentence into words.\n",
    "        \"\"\"\n",
    "        # 5 | tokenize.\n",
    "        def tokenize_sentence(sentence):\n",
    "            return nltk.word_tokenize(sentence)\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda sentence: tokenize_sentence(sentence))\n",
    "\n",
    "    def _remove_stop_words(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Removes the stop words from the tokens.\n",
    "        \"\"\"\n",
    "        def remove_stop_word(words: list) -> list:\n",
    "            \"\"\"  \n",
    "            Remove the stop words from the list of words.\n",
    "\n",
    "            :param words: The list of words in a sentence.\n",
    "            :returns: List of words which are not stop words.\n",
    "            \"\"\"\n",
    "            stop_words = stopwords.words('english')\n",
    "            return [word for word in words if word not in stop_words]\n",
    "            \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda words: remove_stop_word(words))\n",
    "\n",
    "    def _lemmatization(self):\n",
    "        \"\"\" \n",
    "        This function does the following things.\n",
    "        * Lemmatization -- which is converting every word into it's root form.\n",
    "        \"\"\"\n",
    "\n",
    "        def create_lemma(words: list):\n",
    "            \"\"\" \n",
    "            Create the lemma for the list of string.\n",
    "            \"\"\"\n",
    "            lemmetizer = WordNetLemmatizer()\n",
    "            lemmatized_tokens = [lemmetizer.lemmatize(token) for token in words]\n",
    "            return lemmatized_tokens\n",
    "        \n",
    "        self.data[self.column_name] = self.data[self.column_name].apply(lambda row: create_lemma(row))\n",
    "\n",
    "    def run(self, save_preprocessed_dataframe: bool = True,\n",
    "            verbose: bool = True,\n",
    "            fix_typo: bool = True,\n",
    "            lemmatize: bool = True,\n",
    "            remove_stop_words: bool = True,\n",
    "            ) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Actually run the preprocessing pipeline.\n",
    "        Steps which are involved in it.\n",
    "        * 1 - Remove symbols (including numbers, symbols, special characters)\n",
    "        * 2 - Remove links.\n",
    "        * 3 - Case fold -- change all to lower case.\n",
    "        * 4 - Fix typos.\n",
    "        * 5 - tokenization.\n",
    "        * 6 - Remove stop words.\n",
    "        * 7 - lemmatization.\n",
    "        \"\"\"\n",
    "        star_print_length: int = 70\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "        if verbose: print(\"Preprocessing data...\")\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "\n",
    "        # 1 | Remove symbols (including numbers, symbols, special characters).\n",
    "        if verbose: print(\"\")\n",
    "        if verbose: print(\"\\tRemoving numbers, punctuations and special characters...\")\n",
    "        self._remove_symbols()\n",
    "\n",
    "        # 2 | Remove links.\n",
    "        \n",
    "        self._remove_links()\n",
    "        if verbose: print(\"\\tRemoving links...\")\n",
    "        # 3 | Case fold -- change all to lower case.\n",
    "        self._case_fold_to_lower_case()\n",
    "\n",
    "        # 4 | Fix typos.\n",
    "        if fix_typo:\n",
    "            if verbose: print(\"\\tFixing typos...\")\n",
    "            self._fix_typo()\n",
    "\n",
    "        # 5 | tokenization.\n",
    "        if verbose: print(\"\\tTokenization -- Splitting the sentence into tokens...\")\n",
    "        self._tokenization()\n",
    "\n",
    "        # 6 | Remove stop words.\n",
    "        if remove_stop_words:\n",
    "            if verbose: print(\"\\tRemoving stop words...\")\n",
    "            self._remove_stop_words()\n",
    "\n",
    "        # 7 | lemmatization.\n",
    "        if lemmatize:\n",
    "            if verbose: print(\"\\tLemmatization...\")\n",
    "            self._lemmatization()\n",
    "\n",
    "        # save the dataframe, so that we can skip the preprocessing next time.\n",
    "        if save_preprocessed_dataframe:\n",
    "            if verbose: print(\"\\tSaving the dataframe for future use...\")\n",
    "            data.to_pickle(self.file_name)\n",
    "            # pd.read_pickle(file_name)\n",
    "\n",
    "        if verbose: print()\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "        if verbose: print(\"Preprocessing is done successfully.\")\n",
    "        if verbose: print(\"*\"*star_print_length)\n",
    "\n",
    "        return self.data\n",
    "\n",
    "\n",
    "# can be done easily using spacy -- but less control over it\n",
    "# def spacy_tokenizer(doc):\n",
    "#   return [t.lemma_.lower() for t in nlp(doc) if \\\n",
    "#           len(t) > 2 and \\\n",
    "#           not t.is_punct and \\\n",
    "#           not t.is_space and \\\n",
    "#           not t.is_stop and \\\n",
    "#           t.is_alpha]\n",
    "# tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\", lower=False, split='|', oov_token='OOV')\n",
    "\n",
    "# # Import the NLP module which contains Spark NLP and NLU libraries\n",
    "# from johnsnowlabs import nlp\n",
    "# spark = nlp.start(nlp=False)\n",
    "\n",
    "# # Use Norvig model\n",
    "# nlp.load(\"en.spell.norvig\").predict(\"Plaese alliow me tao introdduce myhelf, I am a man of wealth und tiaste\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | word to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the source code:\n",
    "\n",
    "# - **fit_on_texts** Updates internal vocabulary based on a list of texts. This method creates the vocabulary index based on word frequency.\n",
    "#  So if you give it something like, \"The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1;\n",
    "# word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding.\n",
    "# So lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
    "\n",
    "\n",
    "# - **texts_to_sequences** Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and \n",
    "# replaces it with its corresponding integer value from the word_index dictionary. Nothing more, nothing less, certainly no magic involved.\n",
    "\n",
    "\n",
    "# Why don't combine them? Because you almost always fit once and convert to sequences many times.\n",
    "# You will fit on your training corpus once and use that exact same word_index dictionary at train / eval / testing / prediction\n",
    "#time to convert actual text into sequences to feed them to the network. So it makes sense to keep those methods separate.\n",
    "\n",
    "\n",
    "class WordToVectors:\n",
    "    \"\"\"\n",
    "    Converting the word to vectors(numbers). Using pretrained \"word2vec\" model. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data: pd.DataFrame, column_name: str) -> None:\n",
    "        \"\"\" \n",
    "        Initialize the variables.\n",
    "\n",
    "        :param data: The data dataframe to preprocess.\n",
    "        :param column_name: Which column we are doing the preprocessing on.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.column_name = column_name\n",
    "        self.tokenizer = None\n",
    "        # To store the generated vectors.\n",
    "        self.df = pd.DataFrame()\n",
    "        self.token_matrix = None\n",
    "\n",
    "        # Store the word-index for future reference.\n",
    "        self.word_index = None\n",
    "        self.embedding_matrix: np.ndarray = None\n",
    "\n",
    "        # get the inform about the embedding metrix.\n",
    "        self.num_tokens: int = 0\n",
    "        self.embedding_matrix_num_features = 0\n",
    "\n",
    "    def _tokens_to_vectors(self) -> None:\n",
    "        \"\"\" \n",
    "        Generate the vectors from the tokens.\n",
    "        \"\"\"\n",
    "        # First converting the tokens into corresponding numbers.\n",
    "        # oov_token='<OOV>' replaces all out-of-vocabulary words with <OOV>.\n",
    "        if not self.tokenizer:\n",
    "            self.tokenizer = keras.preprocessing.text.Tokenizer(lower=True, oov_token=\"<OOV>\")\n",
    "\n",
    "        # give the tweets texts to the models and fit the texts.\n",
    "        self.tokenizer.fit_on_texts(self.data[self.column_name])\n",
    "\n",
    "        # why token_index -- because the number is actually the index of the word which is stored in the word_index dictionary.\n",
    "        self.token_matrix = self.tokenizer.texts_to_sequences(self.data[self.column_name])\n",
    "\n",
    "        self.word_index = self.tokenizer.word_index\n",
    "\n",
    "    def _sequence_padding(self, max_padding_length: int) -> None:\n",
    "        \"\"\" \n",
    "        Pad the sequence, this is because all the sentence won't be in the same length.\n",
    "        We will decide the max padding length and convert all the sentence to that length.\n",
    "        if the sentence length < max_padding_length --> fill the remaining place with 0.\n",
    "        if the sentence length > max_padding_length --> Trim the sentence.\n",
    "        \"\"\"\n",
    "        self.token_matrix = keras.preprocessing.sequence.pad_sequences(self.token_matrix, \n",
    "                                                                       maxlen=max_padding_length)\n",
    "    \n",
    "    def _generate_n_grams(self) -> pd.DataFrame:\n",
    "        \"\"\" \n",
    "        Generate the n grams.\n",
    "        so the sentence now will be [ '1' ,'2' ,'3', '4' ]\n",
    "\n",
    "        Then, we have to make a n_gram model for good prediction.\n",
    "        this sequence will be splitted into multiple inputs as..\n",
    "\n",
    "                [ '1' ,'2' ,'3', '4' ] -- [\"My\" ,\"name\", \"is\", \"pavithra\"]\n",
    "\n",
    "                [ '1' ,'2' ,'3' ] -- [\"My\" ,\"name\", \"is\"]\n",
    "                \n",
    "                [ '1' ,'2' ] -- [\"My\" ,\"name\"]\n",
    "\n",
    "        :param data: The dataframe.\n",
    "        :param column_name: Where the tokens are present.\n",
    "        :returns: A dataframe with n gram sequence as column values.\n",
    "        \"\"\"\n",
    "        # store the thing in a dataframe.\n",
    "        df_n_grams_target: pd.DataFrame = pd.DataFrame({\"target\": []})\n",
    "        array: list = []\n",
    "        target = []\n",
    "\n",
    "        for row in self.token_matrix:\n",
    "            for index in range(2, len(row) + 1):\n",
    "                array.append(row[:index][:-1])\n",
    "                target.append(row[:index][-1])\n",
    "\n",
    "        # after creating the n-grams split it into features and labels.\n",
    "        del self.df\n",
    "        self.df = pd.DataFrame()\n",
    "        self.token_matrix = array\n",
    "        df_n_grams_target[\"target\"] = target\n",
    "\n",
    "        print(f\"\\tNumber of total sequences --> {len(self.token_matrix)}\")\n",
    "        \n",
    "        return df_n_grams_target\n",
    "    \n",
    "    def _create_embedding_matrix(self) -> None:\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        # load the google pre-build model.\n",
    "        word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/home/pavithra/projects/NLP/GoogleNews-vectors-negative300.bin', binary=True,)\n",
    "\n",
    "        # + 1 to account for padding token. 0 is always reserved for padding in enbedding layer.\n",
    "        self.num_tokens: int = len(self.word_index) + 1\n",
    "\n",
    "        # Initialize a matrix of zeroes of size: vocabulary * embedding dimension.\n",
    "        self.embedding_matrix_num_features: int = 300\n",
    "        self.embedding_matrix = np.zeros((self.num_tokens, self.embedding_matrix_num_features))\n",
    "\n",
    "        for word, i in self.word_index.items():\n",
    "            if word_vectors.has_index_for(word):\n",
    "                self.embedding_matrix[i] = word_vectors[word].copy()\n",
    "    \n",
    "    def run(self, max_padding_length: int):\n",
    "        \"\"\" \n",
    "        Convert the tokens into vectors.\n",
    "\n",
    "        :returns df: The dataframe has the tokens list.\n",
    "        :returns target: The target dataframe.\n",
    "        :returns num_tokens: Number of tokens in the vocabulary.\n",
    "        :returns embedding_matrix_num_features: the dimention of the embedding matrix.\n",
    "        :returns embedding_matrix: The embedding matrix.\n",
    "        \"\"\"\n",
    "        star_print_length: int = 70\n",
    "        print()\n",
    "        print(\"*\"*star_print_length)\n",
    "        print(\"Word to vectorization process in progress...\")\n",
    "        print(\"*\"*star_print_length)\n",
    "\n",
    "        print(\"\\tToken to numbers ...\")\n",
    "        self._tokens_to_vectors()  \n",
    "\n",
    "        # for this models i wanna do n-gram before padding the sequence .\n",
    "        print(\"\\tGenerating the n-grams ...\")\n",
    "        target = self._generate_n_grams()\n",
    "\n",
    "        print(\"\\tpadding the sequence tokens...\")\n",
    "        self._sequence_padding(max_padding_length)\n",
    "\n",
    "        print(\"\\tCreating embedding matrix...\")\n",
    "        self._create_embedding_matrix()\n",
    "        \n",
    "        print(\"\\tSaving the embedding matrix...\")\n",
    "        file_name: str = \"embedding_matrix\" # the extension will be npy\n",
    "        np.save(file_name, self.embedding_matrix, allow_pickle=True)\n",
    "        # to load -- np.load(file_name, allow_pickle=True)\n",
    "\n",
    "        print(f\"Number of tokens in the vocabulary --> {self.num_tokens - 1}\")\n",
    "\n",
    "        print()\n",
    "        print(\"*\"*star_print_length)\n",
    "        print(\"Converted words to vectors.\")\n",
    "        print(\"*\"*star_print_length)\n",
    "\n",
    "        return self.token_matrix, target, self.num_tokens, self.embedding_matrix_num_features, self.embedding_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 | Generate n-grams and labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have sentence in each word represented by numbers, we have to create a n-gram data.\n",
    "we have sentence like \"My name is Pavithra\" --> this will get converted into a sequence with their respective tokens.\n",
    "\n",
    "word_index dictionary for it:\n",
    "    {\n",
    "        \"My\": 1,\n",
    "        \"name\": 2,\n",
    "        \"is\": 3,\n",
    "        \"Pavithra\": 4\n",
    "    }\n",
    "\n",
    "so the sentence now will be [ '1' ,'2' ,'3', '4' ]\n",
    "\n",
    "Then, we have to make a n_gram model for good prediction.\n",
    "this sequence will be splitted into multiple inputs as..\n",
    "\n",
    " [ '1' ,'2' ,'3', '4' ] -- [\"My\" ,\"name\", \"is\", \"pavithra\"]\n",
    "\n",
    " [ '1' ,'2' ,'3' ] -- [\"My\" ,\"name\", \"is\"]\n",
    " \n",
    " [ '1' ,'2' ] -- [\"My\" ,\"name\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data: pd.DataFrame, column_name: str, total_unique_words: int) -> pd.DataFrame:\n",
    "    \"\"\" \n",
    "    One hot encode the dataframe values.\n",
    "\n",
    "    :returns: one-hot encoded dataframe.\n",
    "    \"\"\"\n",
    "    from keras.utils import to_categorical\n",
    "\n",
    "    # NOTE: num_classes -- the max value you will be having in the list + 1. since we need to create that many columns.\n",
    "    # ex: \n",
    "    # [1,3,5] -- output will be having 6 columns --> [0,1,2,3,4,5] {index will always start from 0 in python} -- total classes here is 6.\n",
    "    # \n",
    "    one_hot_matrix = to_categorical(data[column_name], num_classes=total_unique_words)\n",
    "    one_hot_df = pd.DataFrame(one_hot_matrix)\n",
    "\n",
    "    return one_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 | Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(features, num_tokens, embedding_dim, embedding_matrix, padding_length,\n",
    "                weight_decay=1e-4, dropout_rate=0.2, lr=0.001,\n",
    "                num_epoches=20, batch_size=256):\n",
    "    \"\"\" \n",
    "    Create the LSTM model and return the model object.\n",
    "\n",
    "    :param features: tuple of 4 values -- X_train, y_train, X-val, y_val.\n",
    "    :param num_tokens: The number of tokens in the vocabulary.\n",
    "    :param embedding_dim: The dimention of the embedding matrix.\n",
    "    :param padding_length: the length used for padding, this is the input feature length.\n",
    "    :param weight_decay: Delay for the l2 regularization. Default is:1e-4.\n",
    "    :param dropout_rate: Dropout regularization rate.\n",
    "    :param lr: learning rate.\n",
    "    :param num_epoches: The number of epoches,\n",
    "    :param batch_size: batch size to consider for each gradient decent.\n",
    "\n",
    "    :returns:  the DL model.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    # add the embedding layer as the first layer.\n",
    "    embedding_layer = keras.layers.Embedding(\n",
    "                            num_tokens,\n",
    "                            embedding_dim,\n",
    "                            embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "                            input_length=padding_length,\n",
    "                            trainable=True\n",
    "    )\n",
    "\n",
    "    model.add(embedding_layer)\n",
    "\n",
    "    # add a lstm layer and dropout layer to prevent overfittitng.\n",
    "    model.add(keras.layers.LSTM(units=50, \n",
    "                                kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # add a lstm layer and dropout layer to prevent overfittitng.\n",
    "    # model.add(keras.layers.LSTM(50, kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    # model.add(keras.layers.Dropout(dropout_rate))\n",
    "\n",
    "    # model.add(keras.layers.Dense(50, activation='relu',\n",
    "    #                              kernel_regularizer=keras.regularizers.l2(weight_decay)))\n",
    "    \n",
    "    model.add(keras.layers.Dense(num_tokens, activation='softmax'))\n",
    "\n",
    "\n",
    "    # add the optimizers.\n",
    "    opt = keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    # get the callbacks.\n",
    "    # checkpoint_path = \"training_wights\"\n",
    "    # # Create a callback that saves the model's weights\n",
    "    # checkpoint_cb = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "    #                                                 save_weights_only=True,\n",
    "    #                                                 verbose=0,\n",
    "    #                                                 save_freq='epoch')\n",
    "    # patience is the number of epochs to wait before stopping, if the model is not improving.\n",
    "    early_stopping_cb = EarlyStopping(monitor='val_accuracy', verbose=0, patience=3, restore_best_weights=True)\n",
    "\n",
    "\n",
    "    # compile the model.\n",
    "    model.compile(loss=\"categorical_crossentropy\",\n",
    "                  optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    # fit the model.\n",
    "    X_train, y_train, X_val, y_val = features\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        validation_data=(X_val, y_val),\n",
    "                        shuffle=True,\n",
    "                        epochs=num_epoches,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks=[early_stopping_cb])\n",
    "    \n",
    "\n",
    "    # save the model.\n",
    "    model_name: str = f\"saved_model/lstm_lr{lr}_batch_size_{batch_size}.h5\"\n",
    "    model.save(model_name)\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the max sentence length\n",
    "\n",
    "# df_length = pd.DataFrame()\n",
    "# df_length[\"length\"] = data['title'].apply(lambda x: len(x))\n",
    "# max_length = df_length[\"length\"].max()\n",
    "\n",
    "# print(\"max length of the tweets --->\", max_length)\n",
    "# del df_length\n",
    "\n",
    "\n",
    "#-----------------------------------\n",
    "# output\n",
    "#-----------------------------------\n",
    "# max length of the tweets ---> 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of values in train X -----------> 40684\n",
      "Number of values in train y -----------> 40684\n",
      "Number of values in Val X -----------> 7180\n",
      "Number of values in val y -----------> 7180\n"
     ]
    }
   ],
   "source": [
    "COLUMN_NAME: Final[str] = \"title\"\n",
    "MAX_PADDING_LENGTH: Final[int] = 40\n",
    "TOTAL_WORDS_IN_VOC: int = 0\n",
    "\n",
    "data: pd.DataFrame = PreprocessData(data, COLUMN_NAME).run(remove_stop_words=False) # because i need the words like -- \"we, i, ...\"\n",
    "token_matrix, target_df , num_tokens, embedding_matrix_num_features, embedding_matrix = WordToVectors(data, COLUMN_NAME).run(MAX_PADDING_LENGTH)\n",
    "\n",
    "# convert the target into one-hot encoded value.\n",
    "TOTAL_WORDS_IN_VOC = num_tokens  # one extra was added for padding.\n",
    "target_df = one_hot_encode(target_df, \"target\", total_unique_words=TOTAL_WORDS_IN_VOC)\n",
    "\n",
    "# split the input into training and validation set.\n",
    "data = pd.DataFrame(token_matrix)\n",
    "X_train, X_val, y_train, y_val = train_test_split(data, target_df, train_size=0.85, shuffle=True, random_state=1)\n",
    "\n",
    "# train the model.\n",
    "print(f\"\\nNumber of values in train X -----------> {len(X_train)}\")\n",
    "print(f\"Number of values in train y -----------> {len(y_train)}\")\n",
    "print(f\"Number of values in Val X -----------> {len(X_val)}\")\n",
    "print(f\"Number of values in val y -----------> {len(y_val)}\")\n",
    "\n",
    "# model, history = train_model((X_train, y_train, X_val, y_val), num_tokens=TOTAL_WORDS_IN_VOC,\n",
    "#                               embedding_dim=embedding_matrix_num_features,\n",
    "#                               embedding_matrix=embedding_matrix, \n",
    "#                               padding_length=MAX_PADDING_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 40, 300)           2077500   \n",
      "                                                                 \n",
      " lstm_6 (LSTM)               (None, 50)                70200     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 6925)              353175    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2500875 (9.54 MB)\n",
      "Trainable params: 2500875 (9.54 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-04 23:54:02.445975: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 1126946800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159/159 [==============================] - 17s 93ms/step - loss: 7.3656 - accuracy: 0.0328 - val_loss: 6.8843 - val_accuracy: 0.0394\n",
      "Epoch 2/20\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 6.7277 - accuracy: 0.0382 - val_loss: 6.8607 - val_accuracy: 0.0394\n",
      "Epoch 3/20\n",
      "159/159 [==============================] - 14s 89ms/step - loss: 6.6421 - accuracy: 0.0436 - val_loss: 6.8668 - val_accuracy: 0.0475\n",
      "Epoch 4/20\n",
      "125/159 [======================>.......] - ETA: 2s - loss: 6.5692 - accuracy: 0.0495"
     ]
    }
   ],
   "source": [
    "model, history = train_model((X_train, y_train, X_val, y_val), num_tokens=TOTAL_WORDS_IN_VOC,\n",
    "                              embedding_dim=embedding_matrix_num_features,\n",
    "                              embedding_matrix=embedding_matrix, \n",
    "                              padding_length=MAX_PADDING_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
