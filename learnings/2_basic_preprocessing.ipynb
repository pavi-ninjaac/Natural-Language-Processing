{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 | case folding \n",
    "- 2 | stop word removal\n",
    "- 3 | stemming\n",
    "- 4 | Lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "s = \"He told Dr.Lovato that he was done with the tests and would post the results shortly.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 | spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, told, Dr., Lovato, that, he, was, done, with, the, tests, and, would, post, the, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 | case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([t.lower_ for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[He, 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n"
     ]
    }
   ],
   "source": [
    "# You can also apply conditions when generating these views. For example, we can skip case-folding if a token is the start of a sentence.\n",
    "print([t.lower_ if not t.is_sent_start else t for t in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | stop work removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whereas', 'nine', 'wherever', 'anything', 'quite', 're', 'beforehand', 'somehow', 'below', 'at', 'meanwhile', 'from', 'again', 'less', 'everyone', 'do', 'onto', 'somewhere', 'can', 'whom', 'or', 'top', '‘s', 'mostly', 'five', 'ourselves', 'used', 'nor', 'go', 'just', 'much', 'none', 'would', 'she', 'fifty', 'more', 'latterly', '’ll', 'down', 'beyond', 'who', 'bottom', 'yours', 'whoever', 'there', 'well', 'after', 'i', 'those', 'except', 'same', 'what', 'together', 'he', 'rather', 'the', '’d', 'therein', 'within', 'already', 'also', 'a', 'am', 'part', 'out', 'had', 'yourself', 'eight', 'herein', 'seem', 'was', 'our', 'fifteen', 'some', 'whenever', 'eleven', 'whether', 'everywhere', 'everything', 'whence', '’ve', 'never', 'often', 'empty', 'every', 'and', 'sixty', 'one', 'towards', 'besides', \"'ll\", 'around', 'others', 'call', 'own', 'each', 'in', 'whereupon', 'on', 'too', 'by', 'over', 'thereafter', 'alone', 'you', 'seems', 'hereafter', 'which', 'serious', 'should', 'how', 'forty', 'someone', 'nevertheless', 'hereby', 'per', 'really', 'either', 'whereafter', 'thereby', '’m', 'his', 'us', 'wherein', 'nothing', 'where', 'n‘t', 'themselves', 'though', 'is', 'for', 'her', 'however', 'hence', 'here', 'they', 'once', 'cannot', 'seeming', 'side', 'if', 'something', '‘ve', 'six', 'became', 'sometimes', 'any', '’s', 'must', 'but', 'noone', 'twenty', 'being', 'front', 'across', 'put', 'me', 'so', 'has', 'since', 'becomes', 'behind', 'please', 'hereupon', 'thus', 'get', 'might', 'most', '‘ll', 'n’t', 'although', 'whither', 'whole', 'during', 'anyone', 'first', 'them', 'see', 'name', 'thereupon', 'anyhow', 'further', 'could', 'third', 'with', 'former', 'doing', 'as', 'been', 'about', 'almost', 'ten', 'we', 'were', 'without', 'move', 'my', 'keep', 'that', 'myself', 'does', 'herself', 'always', 'otherwise', 'using', 'latter', 'many', 'hers', 'least', '‘d', 'its', 'it', 'ever', 'few', 'several', 'thence', 'say', 'formerly', 'because', 'him', 'all', 'anywhere', \"'d\", 'no', '‘re', 'both', 'why', 'are', 'indeed', 'among', 'various', 'their', 'nobody', 'throughout', 'enough', 'become', 'upon', 'amount', 'else', 'until', 'sometime', 'done', 'whereby', 'beside', 'whatever', '‘m', 'moreover', 'due', 'make', 'thru', 'off', 'these', '’re', 'anyway', 'while', 'via', 'be', 'namely', 'did', 'other', 'not', 'even', 'this', 'still', 'have', 'such', 'two', 'between', 'give', \"'ve\", 'another', 'becoming', \"'m\", 'toward', 'hundred', 'will', 'full', 'last', 'against', 'regarding', 'show', 'neither', \"n't\", 'afterwards', 'up', 'take', 'ca', 'three', 'your', 'twelve', 'than', 'above', 'elsewhere', \"'s\", 'whose', 'mine', 'when', 'next', 'made', 'of', 'to', 'nowhere', 'ours', 'itself', 'under', \"'re\", 'along', 'yourselves', 'yet', 'an', 'himself', 'before', 'therefore', 'seemed', 'unless', 'now', 'back', 'four', 'amongst', 'into', 'perhaps', 'through', 'then', 'may', 'only', 'very'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# spaCy's default stop word list.\n",
    "print(nlp.Defaults.stop_words)\n",
    "print(len(nlp.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[told, Dr., Lovato, tests, post, results, shortly, .]\n"
     ]
    }
   ],
   "source": [
    "print([t for t in doc if not t.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'whereas', 'nine', 'wherever', 'anything', 'quite', 're', 'beforehand', 'somehow', 'below', 'at', 'meanwhile', 'from', 'again', 'less', 'everyone', 'do', 'onto', 'somewhere', 'can', 'whom', 'or', 'top', '‘s', 'mostly', 'five', 'ourselves', 'used', 'nor', 'go', 'just', 'much', 'none', 'would', 'she', 'fifty', 'more', 'latterly', '’ll', 'down', 'beyond', 'who', 'bottom', 'yours', 'whoever', 'there', 'well', 'after', 'i', 'those', 'except', 'same', 'what', 'together', 'he', 'rather', 'the', '’d', 'therein', 'within', 'already', 'also', 'a', 'am', 'part', 'out', 'had', 'yourself', 'eight', 'herein', 'seem', 'was', 'our', 'fifteen', 'some', 'whenever', 'eleven', 'whether', 'everywhere', 'everything', 'whence', '’ve', 'never', 'often', 'empty', 'every', 'and', 'sixty', 'one', 'towards', 'besides', \"'ll\", 'around', 'others', 'call', 'own', 'each', 'in', 'whereupon', 'on', 'too', 'by', 'over', 'thereafter', 'alone', 'you', 'seems', 'hereafter', 'which', 'serious', 'should', 'how', 'forty', 'someone', 'nevertheless', 'hereby', 'per', 'really', 'either', 'whereafter', 'thereby', '’m', 'his', 'us', 'wherein', 'nothing', 'where', 'n‘t', 'themselves', 'though', 'is', 'for', 'her', 'however', 'hence', 'here', 'they', 'once', 'cannot', 'seeming', 'side', 'if', 'something', '‘ve', 'six', 'became', 'sometimes', 'any', '’s', 'must', 'but', 'noone', 'twenty', 'being', 'front', 'across', 'put', 'me', 'so', 'has', 'since', 'becomes', 'behind', 'please', 'hereupon', 'thus', 'get', 'might', 'most', '‘ll', 'n’t', 'although', 'whither', 'whole', 'during', 'anyone', 'first', 'them', 'see', 'name', 'thereupon', 'anyhow', 'further', 'could', 'third', 'with', 'former', 'doing', 'as', 'been', 'about', 'almost', 'ten', 'we', 'were', 'without', 'move', 'my', 'keep', 'that', 'myself', 'does', 'herself', 'always', 'otherwise', 'using', 'latter', 'many', 'hers', 'least', '‘d', 'its', 'it', 'ever', 'few', 'several', 'thence', 'say', 'formerly', 'because', 'him', 'all', 'anywhere', \"'d\", 'no', '‘re', 'both', 'why', 'are', 'indeed', 'among', 'various', 'their', 'nobody', 'throughout', 'enough', 'become', 'upon', 'amount', 'else', 'until', 'sometime', 'done', 'whereby', 'beside', 'whatever', '‘m', 'moreover', 'due', 'make', 'thru', 'off', 'these', '’re', 'anyway', 'while', 'via', 'be', 'namely', 'did', 'other', 'not', 'even', 'this', 'still', 'have', 'such', 'two', 'between', 'give', \"'ve\", 'another', 'becoming', \"'m\", 'toward', 'hundred', 'will', 'full', 'last', 'against', 'regarding', 'show', 'neither', \"n't\", 'afterwards', 'up', 'take', 'my_new_stopword', 'ca', 'three', 'your', 'twelve', 'than', 'above', 'elsewhere', \"'s\", 'whose', 'mine', 'when', 'next', 'made', 'of', 'to', 'nowhere', 'ours', 'itself', 'under', \"'re\", 'along', 'yourselves', 'yet', 'an', 'himself', 'before', 'therefore', 'seemed', 'unless', 'now', 'back', 'four', 'amongst', 'into', 'perhaps', 'through', 'then', 'may', 'only', 'very'}\n",
      "{'whereas', 'nine', 'wherever', 'anything', 'quite', 're', 'beforehand', 'somehow', 'below', 'at', 'meanwhile', 'from', 'again', 'less', 'everyone', 'do', 'onto', 'somewhere', 'can', 'whom', 'or', 'top', '‘s', 'mostly', 'five', 'ourselves', 'used', 'nor', 'go', 'just', 'much', 'none', 'would', 'she', 'fifty', 'more', 'latterly', '’ll', 'down', 'beyond', 'who', 'bottom', 'yours', 'whoever', 'there', 'well', 'after', 'i', 'those', 'except', 'same', 'what', 'together', 'he', 'rather', 'the', '’d', 'therein', 'within', 'already', 'also', 'a', 'am', 'part', 'out', 'had', 'yourself', 'eight', 'herein', 'seem', 'was', 'our', 'fifteen', 'some', 'eleven', 'whether', 'everywhere', 'everything', 'whence', '’ve', 'never', 'often', 'empty', 'every', 'and', 'sixty', 'one', 'towards', 'besides', \"'ll\", 'around', 'others', 'call', 'own', 'each', 'in', 'whereupon', 'my_new_stopword2', 'on', 'too', 'by', 'over', 'thereafter', 'alone', 'you', 'seems', 'hereafter', 'which', 'serious', 'should', 'how', 'forty', 'someone', 'nevertheless', 'hereby', 'per', 'really', 'either', 'whereafter', 'thereby', '’m', 'his', 'us', 'wherein', 'nothing', 'where', 'n‘t', 'themselves', 'though', 'is', 'for', 'her', 'however', 'hence', 'here', 'they', 'once', 'cannot', 'seeming', 'side', 'if', 'something', '‘ve', 'six', 'became', 'sometimes', 'any', '’s', 'must', 'but', 'noone', 'twenty', 'being', 'front', 'across', 'put', 'me', 'so', 'has', 'since', 'becomes', 'behind', 'please', 'hereupon', 'thus', 'get', 'might', 'most', '‘ll', 'n’t', 'although', 'whither', 'whole', 'during', 'anyone', 'first', 'them', 'see', 'name', 'thereupon', 'anyhow', 'further', 'could', 'third', 'with', 'my_new_stopword1', 'former', 'doing', 'as', 'been', 'about', 'almost', 'ten', 'we', 'were', 'without', 'move', 'my', 'keep', 'that', 'myself', 'does', 'herself', 'always', 'otherwise', 'using', 'latter', 'many', 'hers', 'least', '‘d', 'its', 'it', 'ever', 'few', 'several', 'thence', 'say', 'formerly', 'because', 'him', 'all', 'anywhere', \"'d\", 'no', '‘re', 'both', 'why', 'are', 'indeed', 'among', 'various', 'their', 'nobody', 'throughout', 'enough', 'become', 'upon', 'amount', 'else', 'until', 'sometime', 'done', 'whereby', 'beside', '‘m', 'moreover', 'due', 'make', 'thru', 'off', 'these', '’re', 'anyway', 'while', 'via', 'be', 'namely', 'did', 'other', 'not', 'even', 'this', 'still', 'have', 'such', 'two', 'between', 'give', \"'ve\", 'another', 'becoming', \"'m\", 'toward', 'hundred', 'will', 'full', 'last', 'against', 'regarding', 'show', 'neither', \"n't\", 'afterwards', 'up', 'take', 'my_new_stopword', 'ca', 'three', 'your', 'twelve', 'than', 'above', 'elsewhere', \"'s\", 'whose', 'mine', 'when', 'next', 'made', 'of', 'to', 'nowhere', 'ours', 'itself', 'under', \"'re\", 'along', 'yourselves', 'yet', 'an', 'himself', 'before', 'therefore', 'seemed', 'unless', 'now', 'back', 'four', 'amongst', 'into', 'perhaps', 'through', 'then', 'may', 'only', 'very'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Using Spacy 2.0.11, you can update its stopwords set using one of the following:\n",
    "\n",
    "# To add a single stopword:\n",
    "\n",
    "import spacy    \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.Defaults.stop_words.add(\"my_new_stopword\")\n",
    "print(nlp.Defaults.stop_words)\n",
    "\n",
    "# To add several stopwords at once:\n",
    "\n",
    "\n",
    "nlp.Defaults.stop_words |= {\"my_new_stopword1\",\"my_new_stopword2\",}\n",
    "\n",
    "# To remove a single stopword:\n",
    "\n",
    "\n",
    "nlp.Defaults.stop_words.remove(\"whatever\")\n",
    "# \n",
    "# To remove several stopwords at once:\n",
    "\n",
    "\n",
    "nlp.Defaults.stop_words -= {\"whatever\", \"whenever\"}\n",
    "\n",
    "\n",
    "# Note: To see the current set of stopwords, use:\n",
    "\n",
    "print(nlp.Defaults.stop_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 |  Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'he'), ('told', 'tell'), ('Dr.', 'Dr.'), ('Lovato', 'Lovato'), ('that', 'that'), ('he', 'he'), ('was', 'be'), ('done', 'do'), ('with', 'with'), ('the', 'the'), ('tests', 'test'), ('and', 'and'), ('would', 'would'), ('post', 'post'), ('the', 'the'), ('results', 'result'), ('shortly', 'shortly'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print([(t.text, t.lemma_) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'told', 'Dr.', 'Lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'tests', 'and', 'would', 'post', 'the', 'results', 'shortly', '.']\n",
      "['he', 'told', 'dr.', 'lovato', 'that', 'he', 'was', 'done', 'with', 'the', 'test', 'and', 'would', 'post', 'the', 'result', 'short', '.']\n",
      "cared ----> care\n",
      "university ----> univers\n",
      "fairly ----> fair\n",
      "easily ----> easili\n",
      "singing ----> sing\n",
      "sings ----> sing\n",
      "sung ----> sung\n",
      "singer ----> singer\n",
      "sportingly ----> sport\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# EXERCISE: Find out how to intialize the SnowballStemmer, then tokenize\n",
    "# and stem the sentence below.\n",
    "#\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "sen = 'He told Dr. Lovato that he was done with the tests and would post the results shortly.'\n",
    "\n",
    "# Initialize the stemmer here.\n",
    "s = SnowballStemmer(language='english')\n",
    "\n",
    "# Tokenize, stem, and print the tokens.\n",
    "token = word_tokenize(sen)\n",
    "print(token)\n",
    "\n",
    "\n",
    "# stemming \n",
    "print([s.stem(w) for w in token])\n",
    "\n",
    "\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "#list of tokenized words\n",
    "words = ['cared','university','fairly','easily','singing',\n",
    "       'sings','sung','singer','sportingly']\n",
    " \n",
    "#stem's of each word\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    x = snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "     \n",
    "#print stemming results\n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ----> '+e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word = \"i don't wanna go out today. it's not good MR.amal!. i am bored outside\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 | case folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sting lower upper thing for it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 | stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n",
      "word --> i don't wanna go out today. it's not good MR.amal!. i am bored outside\n",
      "[\"n't\", 'wan', 'na', 'go', 'today', '.', \"'s\", 'good', 'MR.amal', '!', '.', 'bored', 'outside']\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('stopwords') # download the stopwords from nltk\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop_words = stopwords.words('english')\n",
    "print(eng_stop_words)\n",
    "print(len(eng_stop_words))\n",
    "\n",
    "\n",
    "word_tokens = word_tokenize(word)\n",
    "# it's simply a list add/remove your own stop words.\n",
    "# filter out using list comprehension.\n",
    "print(\"word -->\", word)\n",
    "print([w for w in word_tokens if w not in eng_stop_words])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 | stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Porter Stemmer: We mostly use this algorithm for its speed, minimal error rate, and simplicity. It is based on the fact that suffixes in the English language are composed of smaller and simpler suffixes. It is only limited to English words. Import as: from nltk.stem.porter import PorterStemmer.\n",
    "rules: https://vijinimallawaarachchi.com/2017/05/09/porter-stemming-algorithm/\n",
    "\n",
    "\n",
    "Snowball stemmer: This is a multilingual stemmer. Thus, it supports other languages. It is more aggressive than the Porter stemmer. Import as: from nltk.stem import SnowballStemmer.\n",
    "\n",
    "Lancaster stemmer: Despite being more aggressive and dynamic than the other stemmers, it is confusing when small words are involved. This stemmer is also less efficient. Import as: from nltk.stem.lancaster import LancasterStemmer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Over-stemming (False positives, precision):  where two separate inflected words are stemmed to the same root, but should not have been\n",
    "Under-stemming (False negatives, recall): where two separate inflected words should be stemmed to the same root, but are they are not.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Unstemmed tokens==== ['i', 'do', \"n't\", 'wan', 'na', 'go', 'out', 'today', '.', 'it', \"'s\", 'not', 'good', 'MR.amal', '!', '.', 'i', 'am', 'bored', 'outside']\n",
      "===Stemmed tokens==== ['i', 'do', \"n't\", 'wan', 'na', 'go', 'out', 'today', '.', 'it', \"'s\", 'not', 'good', 'mr.amal', '!', '.', 'i', 'am', 'bore', 'outsid']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Create a stemmer object\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "stemmed_tokens = [stemmer.stem(token) for token in word_tokens]\n",
    "\n",
    "print(f\"===Unstemmed tokens==== {word_tokens}\")\n",
    "print(f\"===Stemmed tokens==== {stemmed_tokens}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 | lemmetization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/pavithra/nltk_data...\n",
      "[nltk_data] Downloading package wordnet to /home/pavithra/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'do', \"n't\", 'wan', 'na', 'go', 'out', 'today', '.', 'it', \"'s\", 'not', 'good', 'MR.amal', '!', '.', 'i', 'am', 'bored', 'outside']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatizing\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in word_tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 | word frequency matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 1), ('am', 1), ('the', 1), ('bose', 1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "freq_distribution = FreqDist(word_tokenize(\"i am the bose\"))\n",
    "\n",
    "# extract the 10 frequent words in the text\n",
    "freq_distribution.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
