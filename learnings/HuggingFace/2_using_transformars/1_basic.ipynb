{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have to user the same pre-processing step like the transformer pre-trained .\n",
    "So import the Tokenizer according to the model you choose and use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662,\n",
      "        12172,  2607,  2026,  2878,  2166,  1012,   102],\n",
      "       [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(2, 16), dtype=int32, numpy=\n",
      "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n"
     ]
    }
   ],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\",\n",
       " '[CLS] i hate this so much! [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# going through the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModel # specific to tensorflow\n",
    " \n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModel.from_pretrained(checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the output of a model will be high dimentionality vector:\n",
    "\n",
    "A high-dimensional vector?\n",
    "The vector output by the Transformer module is usually large. It generally has three dimensions:\n",
    "\n",
    "- Batch size: The number of sequences processed at a time (2 in our example).\n",
    "- Sequence length: The length of the numerical representation of the sequence (16 in our example).\n",
    "- Hidden size: The vector dimension of each model input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 16, 768)\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs)\n",
    "print(outputs.last_hidden_state.shape) # can also be accessed as outputs[\"last_hidden_state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### models gives logit only not probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFBaseModelOutput(last_hidden_state=<tf.Tensor: shape=(2, 16, 768), dtype=float32, numpy=\n",
       "array([[[-0.17978016,  0.23332867,  0.6320988 , ..., -0.30166638,\n",
       "          0.50082004,  0.14814392],\n",
       "        [ 0.27577713,  0.64971256,  0.31997722, ..., -0.07599576,\n",
       "          0.5136177 ,  0.13292262],\n",
       "        [ 0.90458494,  0.09851397,  0.29497266, ...,  0.33519477,\n",
       "         -0.14074144, -0.6464032 ],\n",
       "        ...,\n",
       "        [ 0.14658965,  0.56606036,  0.32352817, ..., -0.33757502,\n",
       "          0.5099784 , -0.05610833],\n",
       "        [ 0.7500042 ,  0.04872625,  0.17379995, ...,  0.46841478,\n",
       "          0.00296644, -0.6083757 ],\n",
       "        [ 0.05194428,  0.37294865,  0.5223328 , ...,  0.35840556,\n",
       "          0.6500425 , -0.38829762]],\n",
       "\n",
       "       [[-0.29370666,  0.7282564 , -0.14972728, ..., -0.11868107,\n",
       "         -1.0226725 , -0.04215715],\n",
       "        [-0.22063628,  0.93838453, -0.09512515, ..., -0.36431694,\n",
       "         -0.66052234,  0.24069722],\n",
       "        [-0.15360793,  0.8987503 , -0.0727645 , ..., -0.21891795,\n",
       "         -0.85276026,  0.07099412],\n",
       "        ...,\n",
       "        [-0.30174768,  0.90022105, -0.01995095, ..., -0.10816927,\n",
       "         -0.8412146 , -0.0861435 ],\n",
       "        [-0.33384264,  0.96742445, -0.07294402, ..., -0.19517346,\n",
       "         -0.81813306, -0.06339122],\n",
       "        [-0.34538043,  0.8823621 , -0.04263856, ..., -0.09926458,\n",
       "         -0.8328662 , -0.10647446]]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/home/pavithra/projects/Natural-Language-Processing/learnings/HuggingFace/2_using_transformars/0_model_architecture.png\" width=\"1500\" height=\"1000\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the following models are differ in the \"head\" position\n",
    "\n",
    "This particular model will be following a architecture, there are many like that for a specific task:\n",
    "- *Model (retrieve the hidden states)\n",
    "- *ForCausalLM\n",
    "- *ForMaskedLM\n",
    "- *ForMultipleChoice\n",
    "- *ForQuestionAnswering\n",
    "- *ForSequenceClassification\n",
    "- *ForTokenClassification\n",
    "- and others ðŸ¤—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-26 00:36:26.245616: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 93763584 exceeds 10% of free system memory.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    }
   ],
   "source": [
    "# lets use a classcification model\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(inputs)\n",
    "print(outputs.logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1.5606965  1.6122816]\n",
      " [ 4.1692314 -3.346448 ]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[4.0195372e-02 9.5980465e-01]\n",
      " [9.9945587e-01 5.4418371e-04]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(outputs.logits)\n",
    "# you can convert it to softmax.\n",
    "import tensorflow as tf\n",
    "prediction = tf.math.softmax(outputs.logits, axis=-1)\n",
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'NEGATIVE', 1: 'POSITIVE'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
