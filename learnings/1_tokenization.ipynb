{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sentence.\n",
    "s = \"He didn't want to pay $20 for this book.\"\n",
    "doc = nlp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'did', \"n't\", 'want', 'to', 'pay', '$', '20', 'for', 'this', 'book', '.']\n"
     ]
    }
   ],
   "source": [
    "# We can iterate over this **Doc** object and view the tokens.\n",
    "\n",
    "print([t.text for t in doc])\n",
    "\n",
    "# Note how\n",
    "# - \"didn't\" is separated into \"did\"  and \"n't\".\n",
    "# - the currency symbol and amount are separated.\n",
    "# - the period at the end of the sentence is its own token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "# A Doc object is a container of other objects, namely Token and Span objects.\n",
    "print(type(doc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 0), ('did', 1), (\"n't\", 2), ('want', 3), ('to', 4), ('pay', 5), ('$', 6), ('20', 7), ('for', 8), ('this', 9), ('book', 10), ('.', 11)]\n"
     ]
    }
   ],
   "source": [
    "# Access a token's index in a sentence.\n",
    "print([(t.text, t.i) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't\n",
      "<class 'spacy.tokens.span.Span'>\n"
     ]
    }
   ],
   "source": [
    "# Slicing a Doc object returns a Span object.\n",
    "print(doc[0:3])\n",
    "print(type(doc[0:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He didn't want to pay $20 for this book.\n"
     ]
    }
   ],
   "source": [
    "# You can view the original input like so:\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Either the well was very deep, or she fell very slowly, for she \n",
      "had plenty of time as she went down to look about her and to wonder what \n",
      "was going to happen next., First, she tried to look down and make out what \n",
      "she was coming to, but it was too dark to see anything; then she looked at \n",
      "the sides of the well, and noticed that they were filled with cupboards and \n",
      "book-shelves; here and there she saw maps and pictures hung upon pegs.]\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"Either the well was very deep, or she fell very slowly, for she\n",
    "had plenty of time as she went down to look about her and to wonder what\n",
    "was going to happen next. First, she tried to look down and make out what\n",
    "she was coming to, but it was too dark to see anything; then she looked at\n",
    "the sides of the well, and noticed that they were filled with cupboards and\n",
    "book-shelves; here and there she saw maps and pictures hung upon pegs.\"\"\"\n",
    "\n",
    "doc = nlp(s)\n",
    "\n",
    "# Look at individual sentences (there should be two 'Span' objects).\n",
    "print([sent for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n",
      "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = (\"Natural language processing (NLP) is a field \"\n",
    "       \"of computer science, artificial intelligence \"\n",
    "       \"and computational linguistics concerned with \"\n",
    "       \"the interactions between computers and human \"\n",
    "       \"(natural) languages, and, in particular, \"\n",
    "       \"concerned with programming computers to \"\n",
    "       \"fruitfully process large natural language \"\n",
    "       \"corpora. Challenges in natural language \"\n",
    "       \"processing frequently involve natural \"\n",
    "       \"language understanding, natural language\"\n",
    "       \"generation frequently from formal, machine\"\n",
    "       \"-readable logical forms), connecting language \"\n",
    "       \"and machine perception, managing human-\"\n",
    "       \"computer dialog systems, or some combination \"\n",
    "       \"thereof.\")\n",
    "\n",
    "print(sent_tokenize(text))\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"i don't wanna go out today. it's not good MR.amal!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'do', \"n't\", 'wan', 'na', 'go', 'out', 'today', '.', 'it', \"'s\", 'not', 'good', 'MR.amal', '!']\n",
      "===========wordpunct_tokenize==========\n",
      "['i', 'don', \"'\", 't', 'wanna', 'go', 'out', 'today', '.', 'it', \"'\", 's', 'not', 'good', 'MR', '.', 'amal', '!']\n",
      "===========TreebankWordDetokenizer==========\n",
      "i   d o n' t   w a n n a   g o   o u t   t o d a y .   i t' s   n o t   g o o d   M R . a m a l!\n",
      "===========TweetTokenizer==========\n",
      "['i', \"don't\", 'wanna', 'go', 'out', 'today', '.', \"it's\", 'not', 'good', 'MR.amal', '!']\n",
      "=========== MWETokenizer ==========\n",
      "['i', 'do', \"n't\", 'wan', 'na', 'go', 'out', 'today', '.', 'it', \"'s\", 'not', 'good', 'MR.amal', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import (word_tokenize,\n",
    "                           TreebankWordDetokenizer,\n",
    "                           wordpunct_tokenize,\n",
    "                           TweetTokenizer,\n",
    "                           MWETokenizer)\n",
    "\n",
    "# word tokenizer\n",
    "print(word_tokenize(word))\n",
    "\n",
    "# wordpunct_tokenize\n",
    "# This tokenizer splits the sentences into words based on whitespaces and punctuations.\n",
    "print(\"===========wordpunct_tokenize==========\")\n",
    "print(wordpunct_tokenize(word))\n",
    "\n",
    "\n",
    "# TreebankWordDetokenizer\n",
    "print(\"===========TreebankWordDetokenizer==========\")\n",
    "print(TreebankWordDetokenizer().tokenize(word))\n",
    "\n",
    "\n",
    "# When we want to apply tokenization in text data like tweets, the tokenizers mentioned above can’t produce practical tokens.\n",
    "# Through this issue, NLTK has a rule based tokenizer special for tweets. We can split emojis into different words if we need\n",
    "# them for tasks like sentiment analysis.\n",
    "print(\"===========TweetTokenizer==========\")\n",
    "print(TweetTokenizer().tokenize(word))\n",
    "\n",
    "\n",
    "\n",
    "# NLTK’s multi-word expression tokenizer (MWETokenizer) provides a function add_mwe() that allows\n",
    "# the user to enter multiple word expressions before using the tokenizer on the text.\n",
    "# More simply, it can merge multi-word expressions into single tokens.\n",
    "\n",
    "print(\"=========== MWETokenizer ==========\")\n",
    "m = MWETokenizer()\n",
    "m.add_mwe((\"don't\", \"wanna\"))\n",
    "print(m.tokenize(word_tokenize(word)))   # takes input as list of words not sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
