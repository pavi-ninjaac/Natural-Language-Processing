{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, the fetcher retrieves the training subset of the data only.\n",
    "corpus = fetch_20newsgroups(categories=['sci.space'],\n",
    "                            remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils._bunch.Bunch'>\n",
      "593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"\\nAny lunar satellite needs fuel to do regular orbit corrections, and when\\nits fuel runs out it will crash within months.  The orbits of the Apollo\\nmotherships changed noticeably during lunar missions lasting only a few\\ndays.  It is *possible* that there are stable orbits here and there --\\nthe Moon's gravitational field is poorly mapped -- but we know of none.\\n\\nPerturbations from Sun and Earth are relatively minor issues at low\\naltitudes.  The big problem is that the Moon's own gravitational field\\nis quite lumpy due to the irregular distribution of mass within the Moon.\",\n",
       " '\\nGlad to see Griffin is spending his time on engineering rather than on\\nritual purification of the language.  Pity he got stuck with the turkey\\nrather than one of the sensible options.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(corpus))\n",
    "\n",
    "# Number of posts in our dataset.\n",
    "print(len(corpus.data))\n",
    "\n",
    "# View first two posts.\n",
    "corpus.data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavithra/.local/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Like before, if we want to use spaCy's tokenizer, we need\n",
    "# to create a callback. Remember to upgrade spaCy if you need\n",
    "# to (refer to beginnning of file for commentary and instructions).\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# We don't need named-entity recognition nor dependency parsing for\n",
    "# this so these components are disabled. This will speed up the\n",
    "# pipeline. We do need part-of-speech tagging however.\n",
    "unwanted_pipes = [\"ner\", \"parser\"]\n",
    "\n",
    "# For this exercise, we'll remove punctuation and spaces (which\n",
    "# includes newlines), filter for tokens consisting of alphabetic\n",
    "# characters, and return the lemma (which require POS tagging).\n",
    "def spacy_tokenizer(doc):\n",
    "  with nlp.disable_pipes(*unwanted_pipes):\n",
    "    return [t.lemma_ for t in nlp(doc) if \\\n",
    "            not t.is_punct and \\\n",
    "            not t.is_space and \\\n",
    "            t.is_alpha]\n",
    "  \n",
    "# Use the default settings of TfidfVectorizer.\n",
    "vectorizer = TfidfVectorizer(tokenizer=spacy_tokenizer)\n",
    "features = vectorizer.fit_transform(corpus.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9440\n"
     ]
    }
   ],
   "source": [
    "# The number of unique tokens\n",
    "print(len(vectorizer.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(593, 9440)\n"
     ]
    }
   ],
   "source": [
    "# The dimensions of our feature matrix. X rows (documents) by Y columns (tokens).\n",
    "print(type(features))\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 | simple quering the documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity measuring techniques we learned previously can be used here in the same way. In effect, we can query our data using this sequence:\n",
    "\n",
    "Transform our query using the same vocabulary from our fit step on our corpus.\n",
    "Calculate the pairwise cosine similarities between each document in our corpus and our query.\n",
    "Sort them in descending order by score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the query into a TF-IDF vector.\n",
    "query = [\"lunar orbit\"]\n",
    "query_tfidf = vectorizer.transform(query)\n",
    "\n",
    "# Calculate the cosine similarities between the query and each document.\n",
    "# We're calling flatten() here becaue cosine_similarity returns a list\n",
    "# of lists and we just want a single list.\n",
    "cosine_similarities = cosine_similarity(features, query_tfidf).flatten()\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# numpy's argsort() method returns a list of *indices* that\n",
    "# would sort an array:\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.argsort.html\n",
    "#\n",
    "# The sort is ascending, but we want the largest k cosine_similarites\n",
    "# at the bottom of the sort. So we negate k, and get the last k\n",
    "# entries of the indices list in reverse order. There are faster\n",
    "# ways to do this using things like argpartition but this is\n",
    "# more succinct.\n",
    "def top_k(arr, k):\n",
    "  kth_largest = (k + 1) * -1\n",
    "  return np.argsort(arr)[:kth_largest:-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2736328  0.         0.         0.         0.         0.09945553\n",
      " 0.         0.         0.         0.         0.05161722 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.0392964  0.         0.\n",
      " 0.         0.         0.         0.00792869 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.10750917 0.         0.\n",
      " 0.         0.         0.         0.04538261 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.05675789\n",
      " 0.         0.         0.04197194 0.         0.         0.08096857\n",
      " 0.         0.         0.         0.         0.         0.00815123\n",
      " 0.         0.         0.09558718 0.02529751 0.         0.\n",
      " 0.04246228 0.         0.         0.         0.         0.03791222\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.02428377 0.01221538 0.09570231 0.00919254 0.\n",
      " 0.4292246  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.04683803 0.         0.         0.02382188\n",
      " 0.         0.         0.         0.         0.03232624 0.\n",
      " 0.         0.07926538 0.         0.08155756 0.         0.\n",
      " 0.         0.         0.07583789 0.         0.         0.\n",
      " 0.04361727 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05679512 0.         0.         0.         0.03348621\n",
      " 0.04799273 0.03523014 0.         0.05752858 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.06825601 0.         0.         0.03443122 0.         0.\n",
      " 0.15135536 0.         0.         0.         0.         0.\n",
      " 0.00893802 0.         0.         0.         0.         0.09238021\n",
      " 0.         0.         0.         0.         0.         0.05126984\n",
      " 0.         0.06510093 0.04084089 0.         0.         0.\n",
      " 0.0067666  0.         0.         0.         0.         0.02357085\n",
      " 0.         0.         0.         0.         0.09081073 0.\n",
      " 0.05135637 0.         0.         0.04490695 0.03910072 0.\n",
      " 0.         0.         0.         0.         0.         0.13780164\n",
      " 0.         0.         0.04702505 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.0134042  0.05940414 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.11393948 0.         0.02182584 0.47855355 0.         0.17471517\n",
      " 0.         0.         0.02531391 0.07393686 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.17642802 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.03445866 0.         0.         0.         0.\n",
      " 0.         0.02919164 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.09566568 0.02955542 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0360768  0.0310056  0.         0.         0.\n",
      " 0.19486489 0.         0.         0.         0.         0.\n",
      " 0.04601201 0.12540023 0.         0.         0.13202071 0.\n",
      " 0.         0.03631657 0.         0.         0.08547915 0.\n",
      " 0.         0.         0.         0.         0.         0.13358867\n",
      " 0.         0.         0.         0.08321783 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.07165935\n",
      " 0.01271728 0.         0.         0.         0.06431597 0.\n",
      " 0.         0.03996606 0.         0.         0.         0.\n",
      " 0.         0.03633953 0.05170227 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.04255216 0.\n",
      " 0.         0.16758348 0.0602143  0.         0.         0.\n",
      " 0.0691536  0.10679413 0.         0.         0.04741599 0.\n",
      " 0.02508564 0.         0.         0.         0.         0.\n",
      " 0.         0.03467896 0.         0.         0.         0.\n",
      " 0.12416731 0.         0.         0.         0.         0.\n",
      " 0.         0.02565133 0.         0.         0.06002124 0.1613912\n",
      " 0.         0.03440789 0.         0.         0.         0.06486125\n",
      " 0.         0.         0.         0.         0.09451514 0.04519899\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.15167963 0.         0.         0.03446473\n",
      " 0.         0.         0.         0.         0.05190307 0.\n",
      " 0.         0.         0.         0.0702391  0.01410765 0.\n",
      " 0.         0.         0.         0.         0.         0.11908586\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.0451296  0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.11587686 0.         0.\n",
      " 0.05612797 0.16687422 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19125175\n",
      " 0.         0.         0.         0.08750547 0.03358098 0.\n",
      " 0.         0.02981047 0.         0.04542887 0.         0.\n",
      " 0.00638335 0.05828761 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.01679617\n",
      " 0.         0.         0.         0.         0.17212588 0.\n",
      " 0.         0.         0.06912576 0.         0.         0.\n",
      " 0.         0.         0.         0.05242798 0.         0.\n",
      " 0.         0.08809991 0.05638883 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.00844962 0.\n",
      " 0.         0.         0.         0.12050159 0.         0.\n",
      " 0.         0.         0.         0.05108545 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.08819015]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[249 108   0 312 509]\n"
     ]
    }
   ],
   "source": [
    "# So for our query above, these are the top five documents.\n",
    "top_related_indices = top_k(cosine_similarities, 5)\n",
    "print(top_related_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.47855355 0.4292246  0.2736328  0.19486489 0.19125175]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarities[top_related_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Actually, Hiten wasn't originally intended to go into lunar orbit at all,\n",
      "so it indeed didn't have much fuel on hand.  The lunar-orbit mission was\n",
      "an afterthought, after Hagoromo (a tiny subsatellite deployed by Hiten\n",
      "during a lunar flyby) had a transmitter failure and its proper insertion\n",
      "into lunar orbit couldn't be positively confirmed.\n",
      "\n",
      "It should be noted that the technique does have disadvantages.  It takes\n",
      "a long time, and you end up with a relatively inconvenient lunar orbit.\n",
      "If you want something useful like a low circular polar orbit, you do have\n",
      "to plan to expend a certain amount of fuel, although it is reduced from\n",
      "what you'd need for the brute-force approach.\n"
     ]
    }
   ],
   "source": [
    "# Top match.\n",
    "print(corpus.data[top_related_indices[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# still not so perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we have the beginnings of a simple search engine but we're a far cry from competing with commercial off-the-shelf search engines, let alone Google.\n",
    "\n",
    "- For each query, we're scanning through our entire corpus, but in practice, you'll want to create an inverted index. Search applications such as Elasticsearch do that under the hood.\n",
    "- You'd also want to evaluate the efficacy of your search using metrics like precision and recall.\n",
    "- Document ranking also tends to be more sophisticated, using different ranking functions like Okapi BM25. With major search engines, ranking also involves hundreds of variables such as what the user searched for previously, what do they tend to click on, where are they physically, and on and on. These variables are part of the \"secret sauce\" and are closely guarded by companies.\n",
    "- Beyond word presence, intent and meaning are playing a larger role.\n",
    "\n",
    "\n",
    "Information Retrieval is a huge, rich topic and beyond search, it's also key in tasks such as question-answering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
